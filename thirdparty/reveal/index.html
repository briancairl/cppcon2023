<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>CppCon 2023</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/custom.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<!-- slide -->
				<section data-background="slides/cppcontitle.png">
					<aside class="notes">
						<p>Hello everyone.</p>
						<p>My name is Brian Cairl and I am presenting: Cache-friendly design in robot path planning</p>
					</aside>
				</section>


				<!-- slide -->
				<section data-transition="fade">
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<ul align="left">
								  <li>This is my first time at CppCon!</li>
								  <li>Regularly program in C++ for robotics</li>
								</ul>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>This is my first live cppcon.</p>
							<p>Last year I attended remotely, so I'm excited to be here in person.</p>
							<p>I've programmed in C++ on a daily basis as a robotics software developer for the past 7 years.</p>
							<p>Though, I've been a C++ programmer since 2012 or so.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<div align="left">
									<h4 style="color: white">I've worked at a few robotics startups:</h4>
									<ul align="left">
										<li>Fetch Robotics (2016-2022)</li>
										<li>Thirdwave.ai (2022-2023)</li>
										<li>Tortuga AgTech (present)</li>
									</ul>
								</div>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>I've worked on mostly mobile robot systems which operate in warehouses.</p>
							<p>But as of a few months ago I started working at a robot agriculture company called Tortuga AgTech, so first time in 7 years, I'm finely getting some fresh air.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<div align="left">
									<h4 style="color: white">Mostly doing:</h4>
									<ul align="left">
										<li>General architecture and frameworks around robot systems</li>
										<li>Navigation and perception systems</li>
									</ul>
								</div>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>I typically write broader robot systems code.</p>
							<p>But I've focused on mostly algorithms and architecture around robot navigation and perception.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<div align="left">
									<h4 style="color: white">Other interests:</h4>
									<ul>
										<li>Game engines</li>
										<li>Meta-programming</li>
									</ul>
								</div>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>Besides coding in C++ for work, I also code for fun.</p>
							<p>In 1000 years I'll finish a game engine.</p>
							<p>And to procrastinate I tool around template meta-programming nonsense.</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2 style="color: white">Robot path planning</h2>
						<aside class="notes">
							<p>To parse the title of this talk backwards, lets start with: robot path planning</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Robot path planning</h2>
						<h2 style="color: white">Path planning</h2>
						<aside class="notes">
							<p>Or more generally just: path planning</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Path planning</h2>
						<ul>
						How an (autonomous) agent figures out how to get from one location to another before actually moving.
						</ul>
						<div class="r-stack">
							<img src="slides/robot_path_planning.png" width="500" height="auto">
						</div>
						<aside class="notes">
							<p>Refers to how an (autonomous) agent figures out how to get from one location to another before actually moving.</p>
							<p>Within robotics this extends to a variety of sub-domains.</p>
							<p>We plan paths for 9-jointed robot arms to move car parts to and from a conveyor belt.</p>
							<p>Just as we plan paths for our robot vacuum cleaners to make their way through our houses.</p>
							<p>But the concept of path planning, obviously, extends outside the domain of robotics.</p>
							<p>Many of the same planning algorithms used in robots finds its place in games; online map services, like google maps; etc.</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<aside class="notes">
							<p>So now we're onto the the slightly more nebulous and philosophical part of the title: cache friendly design</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
						Program design focused on optimizing code by avoiding pathological affects of memory access through the caching structure of a computer system.
						</ul>
						<aside class="notes">
							<p>Cache friendly design refers to: program design guided by attempts to prevent the CPU from slowing down while accessing data in memory</p>
							<p>And because most CPUs we actually use are faster than our memory hardware: we are accessing that data through some complex pipeline of connected cache levels which sit between the CPU and main memory.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div>
							<blockquote>Modern memory pipelines are so complex you are basically optimizing for the cache</blockquote>
							- Random person on StackOverflow
						</div>
						<aside class="notes">
							<p>And these are indeed complex and important</p>
							<p>This guy said so</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/robot_target_system.png">
						</div>
						<aside class="notes">
							<p>Robots have brains made of computers</p>
							<p>Most of the robots I've ever worked on use the same CPU hardware that sits in most high-end laptops</p>
							<p>So at least in the domain of mobile robotics I'm covering a lot of ground by claiming that, as far as hardware is concerned: the CPUs we use to execute high-level software like path-planners, general isn't exotic</p>
							<p>And we are optimizing for the same targets that most people working in a non-embedded systems land are optimizing for.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/robot_program_architecture.png">
						</div>
						<aside class="notes">
							<p>A typical robot system might have some high-level architecture that looks like this</p>
							<p>Where we have a bunch of non-hard-real-time applications running together in user space</p>
							<p>They are talking to each other with some inter-process message-passing protocol</p>
							<p>A popular choice for this sort of structure is ROS (the Robot operating system), which is not actually an OS</p>
							<p>And they are interfaced with some lower level, probably but not definitely real-time systems for actually getting the robot to move and; reacting safely and deterministically around critical things like human passersby</p>
							<p>At least we hope</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/cache.png">
						</div>
						<aside class="notes">
							<p>All that to say: at least for the most brain-like part of the system, we are running on a consumer-grade CPU</p>
							<p>With a consumer-grade memory pipeline</p>
							<p>That is probably organized something like this where we have small, but fast caches close to our cores</p>
							<p>And some large, slower cache interfaced with main memory</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
						</ul>
						<aside class="notes">
							<p>Any time one of our high-level programs reads or writes data to memory, it is doing so through the cache</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>	
						</ul>
						<aside class="notes">
							<p>Any time one of our high-level programs executes instructions, it is first reading them from the cache</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>	
							<li>Besides the data the CPU needs now, the cache is populated with data likely to be accessed soon (pre-fetching)</li>
						</ul>
						<aside class="notes">
							<p>In order to reduce the amount of times that we need to reach into slow, main memory, the cache is populated with possibly relevant data besides the data the program is currently accessing</p>
							<p>This is called pre-fetching</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>
							<li>Besides the data the CPU needs now, the cache is populated with data likely to be accessed soon (pre-fetching)</li>
							<li>Programs may attempt to access data that is not in the cache or has been removed (evicted) from the cache</li>
						</ul>
						<aside class="notes">
							<p>When things are already in the cache, the CPU is happy</p>
							<p>But sometimes data a program might need has already been removed from the cache, or it wasn't in the cache to begin with</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>
							<li>Besides the data the CPU needs now, the cache is populated with data likely to be accessed soon (pre-fetching)</li>
							<li>Programs may attempt to access data that is not in the cache or has been removed (evicted) from the cache</li>
							<li>If a program attempts to access something that not in the cache, this is a <em style="color:red">cache miss</em></li>	
						</ul>
						<aside class="notes">
							<p>This is called a cache miss</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/cache_miss.png">
						</div>
						<aside class="notes">
							<p>Cache misses can happen at each level of the cache</p>
							<p>If data isn't available in an L1 cache line, the L2 cache is checked, and so on and so forth</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
						</ul>
						<aside class="notes">
							<p>The cache exists to bridge the gap between the difference in speed between CPU and memory hardware</p>
							<p>And on the happy path, this obviously works</p>
							<p>But cache misses potentially add extra latency to program execution</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
						</ul>
						<aside class="notes">
							<p>Accessing data at each cache level, from L1 down to main memory, is considerably slower than the previous level</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
							<li>CPUs can run out of things to do while waiting for the the next cache line from memory</li>	
						</ul>
						<aside class="notes">
							<p>So a bad cache miss where the data isn't in any cache can cause the CPU to wait</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
							<li>CPUs can run out of things to do while waiting for the the next cache line from memory</li>
							<li>This is called a CPU stall</li>
						</ul>
						<aside class="notes">
							<p>This is referred to as a CPU stall</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
							<li>CPUs can run out of things to do while waiting for the the next cache line from memory</li>
							<li>This is called a CPU stall</li>
							<li>There are mitigations, e.g. out-of-order execution</li>
						</ul>
						<aside class="notes">
							<p>There are mitigations beyond our control, such as out-of-order execution, that allow a CPU can run instructions while waiting on a cache-miss to be satisfied</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<h3 style="color: white">Optimizes how a program uses data already in the cache.</h3>
						<aside class="notes">
							<p>But we can certainly attempt to write code in such a way that makes the best use of the cache pipeline</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<h1 style="color: white">Minimizes cache misses!</h1>
						<aside class="notes">
							<p>And in order to do that, we will have to minimize the number of cache-misses that occur during the execution of our programs</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<aside class="notes">
							<p>With all that background in mind: some high level goals of this talk</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
						</ul>
						<aside class="notes">
							<p>We want to be able to implemented path planning code for typical robot systems that is performant, and, for the sake of this take, specifically cache-performant</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
							<li>Do so using the STL</li>
						</ul>
						<aside class="notes">
							<p>We want to be able to do so using facilities that are readily available to C++ programmers out of the box: the STL</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
							<li>Do so using the STL</li>
							<li>Try to keep our choices simple</li>
						</ul>
						<aside class="notes">
							<p>We will try to keep these choices simple</p>
							<p>Namely, we will do our best with what we have without touching custom memory allocators</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
							<li>Do so using the STL</li>
							<li>Try to keep our choices simple</li>
							<li>Measure</li>
						</ul>
						<aside class="notes">
							<p>We will make some choices guided by measurement</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<aside class="notes">
							<p>To kick off an implementation journey, lets focus on a motivating problem</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3 style="color: white">Robots typically need to move.</h3>
						<aside class="notes">
							<p>We want our robots to move</p>
							<p>A noble goal</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3 style="color: white">Mobile Robot Systems</h3>
						<aside class="notes">
							<p>Lets focus on mobile robot systems</p>
							<p>Mobile means they definitely should be moving</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>Warehouse robots: forklifts, pallet-movers, etc.</li>
						</ul>
						<aside class="notes">
							<p>Lets focus even more specifically on robots that can pretend they live in a 2D world</p>
							<p>Like autonomous forklifts moving pallets around</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>Warehouse robots: forklifts, pallet-movers, etc.</li>
							<li>Home robots: vacuums, etc.</li>
						</ul>
						<aside class="notes">
							<p>Home robots moving around your house, moving along some hopefully optimal dust collecting paths</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>Warehouse robots: forklifts, pallet-movers, etc.</li>
							<li>Home robots: vacuums, etc.</li>
							<li>Outdoor robots: tractors, berry-pickers, delivery platforms, etc.</li>
						</ul>
						<aside class="notes">
							<p>Or even outdoor robots like robot tractors; berry-pickers; or robot delivery systems</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>These robots operate in highly dynamic environments.</li>
						</ul>
						<aside class="notes">
							<p>Even though these robots hallucinate a lower dimensional space, moving around is a hard problem because the environment they work in is chaotic</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>These robots operate in highly dynamic environments.</li>
							<li>Robots might be informed about blockages local observation or some centralized coordination system.</li>
						</ul>
						<aside class="notes">
							<p>They might experience blockages while driving that they directly observe or are informed about</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>These robots operate in highly dynamic environments.</li>
							<li>Robots might be informed about blockages local observation or some centralized coordination system.</li>
							<li>A robot might need to figure out how to move multiple times while making its way to a goal.</li>
						</ul>
						<aside class="notes">
							<p>So they likely need to be reactive with how they come up with new plans towards a goal they are moving to.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<div class="r-stack">
							<img src="slides/robot_navigation_system.png">
						</div>
						<aside class="notes">
							<p>A hypothetical state machine for a navigating robot system might look like this.</p>
							<p>A robot is initialized with some information about its environment, typically in the form of a map and/or a graph.</p>
							<p>It gets commands to go somewhere else.</p>
							<p>It comes up with some high-level reference plan all the way to its goal, that we will refer to as a global plan</p>
							<p>And then it follows that global plan as a reference while it uses some other lower-level planner/optimizer/controller to make adjustments around obstacles</p>
							<p>When the local control systems can't figure out how to react, the system will likely drop back to the global planner for a new global plan</p>
							<p>And so on and so forth until the robot gets to its intended goal location</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
						</ul>
						<aside class="notes">
							<p>Coming up with these global plans is a relatively infrequent occurrence given everything else going on in the system at a given time</p>
							<p>But when they happen, they are typically require a lot of resources</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
						</ul>
						<aside class="notes">
							<p>This only gets worse for larger planning state-spaces</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
							<li>We are concerned about computation time as it affects overall system latency stack-up.</li>
						</ul>
						<aside class="notes">
							<p>And we are concerned with minimizing the amount of time it takes to come up with a global plan as it factors into how reactive our system is</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
							<li>We are concerned about computation time as it affects overall system latency stack-up.</li>
							<li>When we tell a robot to do something, we want it moving quickly.</li>
						</ul>
						<aside class="notes">
							<p>We typically want our robots to look intelligent, so they have to be snappy about re-routing</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
							<li>We are concerned about computation time as it affects overall system latency stack-up.</li>
							<li>When we tell a robot to do something, we want it moving quickly.</li>
							<li style="color: cyan">Fast planning algorithms == less time robots are sitting around.</li>
						</ul>
						<aside class="notes">
							<p>And more importantly: is money, chop-chop, the robot's cant be sitting around</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						</ul>
						<aside class="notes">
							<p>Lets look at a typical environment we would want to move through</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						<h3 style="color: white">Robots operating in a very big warehouse.</h3>
						<aside class="notes">
							<p>An easy example that we can't seem to get away from these days is some mega-warehouse butlered by mobile robots</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility.png" width="1500", height="auto">
						</div>
						<aside class="notes">
							<p>Here is a top-down map of a make-believe distribution facility</p>
							<p>Its probably quite hard to see, but it has a bunch of repeated areas with shelves probably holding beans or protein powder or whatever folks order from a mega-corp</p>
							<p>And at the bottom and top are some areas where a robot might take said product to be packaged for shipping</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility.png" width="1500", height="auto">
						</div>
						<div align="center">
							This is a map of a make-believe distribution facility that is:
							<lu align>
								<li>0.8 km long</li>
								<li>0.4 km deep</li>
							</lu>
						</div>
						<aside class="notes">
							<p>This facility is fairly big, but not the biggest</p>
							<p>Its 0.8 by 0.4 km where each pixel of this image represents 5cm of space</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility_graph.png">
						</div>
						<lu>This is a spatial graph extracted from the map.</lu>
						<aside class="notes">
							<p>And here is a spatial graph extracted from the map using a fancy little python script which did some image processing</p>
							<p>All for naught because its probably hard for you to see it</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility_graph_zoomed.png">
						</div>
						<lu>This is a spatial graph informs the robot about good "roadways" to travel along.</lu>
						<aside class="notes">
							<p>So zooming in a bit we can see things a bit better</p>
							<p>The graph effectively establishes some nice, central pathways between aisles and shelves that the robot can plan along</p>
							<p>We can assume for now that the paths we can extract from this graph are all kinematically sane and traversable by our robot system</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Operating environment, a priori</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility_graph_path.png">
						</div>
						<lu>A robot could used this graph to make global plans using this graph to get around the warehouse.</lu>
						<aside class="notes">
							<p>And to get from place to place, a robot would, compute paths from its current location to somewhere else in an on-line sense</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-transition="fade">
						<h2>Why not just pre-compute all paths?</h2>
						<div class="r-stack">
							<img src="slides/no_precompute.png">
						</div>
						<aside class="notes">
							<p>Just to get this out of the way: why not just also pre-compute all the possible paths the robot can take as well?</p>
							<p>I mean, probably the best we can do in terms of memory performance is just load sequential blocks of data from memory</p>
							<p>And it it was simply a matter of traveling from vertex to vertex, we probably could for even relatively large graphs</p>
						</aside>
					</section>
					<section data-transition="fade">
						<h2>Why not just pre-compute all paths?</h2>
						<div class="r-stack">
							<img src="slides/no_precompute_2.png">
						</div>
						<aside class="notes">
							<p>But the environment is dynamic. Edges become inaccessible due to blockages we can't know about ahead time</p>
							<p>This changes the topology of the graph</p>
							<p>As a result, pre-computing would lead to a combinatorial explosion of a lookup table when you have to consider every possible edge as a gate</p>
							<p>So, practically speaking, we will need to run some kind of on-line search unless our graph is very small</p>
							<p>As a sort of throw-away remark, we could still use precomputed paths as a heuristic to speed up our algorithms, but lets put that aside</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What do we do?</h2>
						<aside class="notes">
							<p>So what do we do?</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2 style="color: white">Shortest-path search algorithms</h2>
						<aside class="notes">
							<p>We'll employ a shortest path search algorithm</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3 style="color: white">Sampling-based</h3>
						<aside class="notes">
							<p>Some common ones used in the field are sampling based, where we basically make structured-random guesses to fill space based on the robots kinematics or other constraints.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Sampling-based</h3>
						<lu align="left">
							<li>Rapidly-exploring random trees (RRT)</li>
						</lu>
						<aside class="notes">
							<p>Just to name some: these include RRT: Rapidly-exploring random trees</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Sampling-based</h3>
						<lu align="left">
							<li>Rapidly-exploring random trees (RRT)</li>
							<li>Probabilistic road maps (PRM)</li>
						</lu>
						<aside class="notes">
							<p>These include PRM: Probabilistic road maps</p>
						</aside>
					</section>


					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3 style="color: white">Search-based</h3>
						<aside class="notes">
							<p>The flavor we will focus on is the search based variety.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
						</lu>
						<aside class="notes">
							<p>The canonical example is Dijkstra's algorithm.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
						</lu>
						<aside class="notes">
							<p>A*, which is effectively a superset-form of Dijstra's.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
							<li>Lifelong planning A* (LPA*)</li>
						</lu>
						<aside class="notes">
							<p>LPA*</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
							<li>Lifelong planning A* (LPA*)</li>
							<li>D*</li>
						</lu>
						<aside class="notes">
							<p>D*</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
							<li>Lifelong planning A* (LPA*)</li>
							<li>D*</li>
							<li>etc.</li>
						</lu>
						<aside class="notes">
							<p>and so on</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li style="color: red;">Dijkstra's algorithm</li>
							<li style="color: red;">A*</li>
							<li style="color: grey;">Lifelong planning A* (LPA*)</li>
							<li style="color: grey;">D*</li>
							<li style="color: grey;">etc.</li>
						</lu>
						<aside class="notes">
							<p>But the planning algorithm we will focus on is Dijkstra's algorithm.</p>
							<p>The reason for this is that, anecdotally, its sort of a first-pick in robot systems where you can know the graph you are planning over ahead of time.</p>
							<p>So it covers a lot of practical application ground.</p>
							<p>Its a relatively simple algorithm so show as C++.</p>
							<p>And any performance optimizations we can make will also apply to A* because of how similar these algorithms are.</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<h2>Dijkstra's algorithm (early stopping)</h2>
					<div class="r-stack">
						<img src="slides/dijkstras_fast.gif" height="400", width="400">
					</div>
					<aside class="notes">
						<p>So here is that semi famous wiki-pedia graphic for Dijkstra's</p>
						<p>Here, the spatial graph is embedded upon some kind of uniform lattice structure, like a grid</p>
						<p>And we see that the search proceeds from some start point until a goal vertex is reached</p>
						<p>We can call this an "early stopping" variant of Dijkstra's since the algorithm stops at a specific goal vertex</p>
						<p>Though the more common variant will find optimal paths to all other nodes, from the start if you just let it run until all vertices have been visited</p>
					</aside>
				</section>


				<!-- slide -->
				<section data-markdown>
					 <textarea data-template>
## Dijkstra's algorithm (early stopping)

```r [1|3-6|8|9-10|11-13|15-16|17-19|21]
function Dijkstras(Graph, source, target):

  for each vertex v in Graph.Vertices:
    prev[v] ← UNDEFINED                                 # predecessors of v (visited set)
  create (vertex, vertex, dist) priority queue Q        # create a priority queue
  Q.add_with_priority((start, start, 0))                # add start vertex to priority queue

  while Q is not empty:
    (p,u,dist_u) ← Q.extract_min()                      # remove next best vertex (u) and its
                                                        # predecesssor (p)
    if prev[u] is not UNDEFINED:                        # if we have visted (u), ignore
      continue
    prev[u] ← p                                         # otherwise, set predecessor of (u)

    if u is target:                                     # end if we are at the target vertex
      break
    for each neighbor v of u with prev[v] is UNDEFINED:
      dist_v ← dist_u + Graph.Edge(u, v)                # compute distance to (v)
      Q.add_with_priority((u, v, dist_v))               # enqueue (v,u,dist)

  return prev[]                                         # return predecessors to recover path
```
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<h2>A* (early stopping)</h2>
					<div class="r-stack">
						<img src="slides/astar_fast.gif" height="400", width="400">
					</div>
					<aside class="notes">
						<p>And here is another semi famous wiki-pedia graphic for A*</p>
						<p>We see that, compared to Dijkstra's, the algorithm has to visit less vertices to find a path to the goal</p>
						<p>This is because it uses a heuristic function to estimate the total cost to the goal. which changes the order in which vertices are evaluated</p>
						<p>As long as this heuristic underestimates the cost to the goal, A* will give us the same optimal paths as Dijkstra's for less computational work</p>
						<p>So the first optimization one can make when using Dijkstra's is: just use A* and pick a good heuristic</p>
						<p>But here we are interested in optimizing the work the CPU is doing in a mostly orthogonal domain</p>
						<p>And I'm claiming that the optimizations we make will naturally extend to A*</p>
						<p>To prove it, here is the algorithm</p>
					</aside>
				</section>


				<!-- slide -->
				<section data-markdown>
					 <textarea data-template>
## A* (early stopping)

```r [18]
function Astar(Graph, source, target):

  for each vertex v in Graph.Vertices:
    prev[v] ← UNDEFINED                                 # predecessors of v (visited set)
  create (vertex, vertex, dist) priority queue Q        # create a priority queue
  Q.add_with_priority((start, start, 0))                # add start vertex to priority queue

  while Q is not empty:
    (p,u,dist_u) ← Q.extract_min()                      # remove next best vertex (u) and its
                                                        # predecesssor (p)
    if prev[u] is not UNDEFINED:                        # if we have visted (u), ignore
      continue
    prev[u] ← p                                         # otherwise, set predecessor of (u)

    if u is target:                                     # end if we are at the target vertex
      break
    for each neighbor v of u with prev[v] is UNDEFINED:
      dist_v ← dist_u + Graph.Edge(u, v) + H(v)         # compute distance to (v) + heuristic
      Q.add_with_priority((u, v, dist_v))               # enqueue (v,u,dist)

  return prev[]                                         # return predecessors to recover path
```
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
							<li>Measure cache performance</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
							<li>Measure cache performance</li>
							<li>Incrementally improve performance with better choices</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
							<li>Measure cache performance</li>
							<li>Incrementally improve performance with better choices</li>
							<li>Keep in mind that any improvements we make will also apply to A* or similar search algorithms.</li>
						</lu>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What do we need?</h2>
					</section>
					<section data-auto-animate>
						<h2>What do we need?</h2>
						<h1 style="color: white">The search algorithm!</h1>
					</section>
				</section>


				<!-- slide -->
				<section data-markdown>
					 <textarea data-template>
Here is Dijkstra's algorithm as C++ code
```c++ [|1-2|4|6|8|10-13|14-17|19-22|23-32|34]
template<SearchContext C, SearchGraph G>
bool search(C& ctx, const G& graph, VertexID start, VertexID goal)
{
  ctx.reset(graph, start, goal);

  while (ctx.is_queue_not_empty())
  {
    const auto [from, to, to_dist] = ctx.dequeue();
    
    if (ctx.is_visited(to))
    {
      continue;
    }
    else
    {
      ctx.mark_visited(to, from);
    }
    
    if (ctx.is_terminal(to))
    {
      return true;
    }
    else
    {
      graph.for_each_edge(
        to,
        [&ctx, to, to_dist](VertexID v, const EdgeProperties& edge) mutable {
        if (edge.valid and !ctx.is_visited(v)) {
          ctx.enqueue(to, v, edge.dist + to_dist);
        }
      }); 
    }
  }
  return false;
}
```
					</textarea>
				</section>

				<!-- slide -->
				<section data-markdown>
					 <textarea data-template>
Here is the code to recover a path on a successful search
```c++ [1-2|4|7-10|11-15|17]
template<typename OutputIteratorT, SearchContext C>
OutputIteratorT get_reverse_path(OutputIteratorT out, const C& ctx, VertexID from)
{
  (*out) = from;
  while (true)
  {
    if (const auto to = ctx.predecessor(from); to == from)
    {
      break;
    }
    else
    {
      (*out) = to;
      from = to;
    }
  }
  return out;
}
```
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3 style="color: white">Something to hold the graph!</h3>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold the graph!</h3>
						<lu align="left">
							<li>Vertex adjacencies (edges)</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold the graph!</h3>
						<lu align="left">
							<li>Vertex adjacencies (edges)</li>
							<li>Vertex properties (locations, etc.)</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold the graph!</h3>
						<lu align="left">
							<li>Vertex adjacencies (edges)</li>
							<li>Vertex properties (locations, etc.)</li>
							<li>Edge properties (distance, validity, etc.)</li>
						</lu>
					</section>
				</section>


				<!-- slide -->
				<section data-auto-animate data-markdown>
					 <textarea data-template>
## What do we need?
```c++
template <typename T>
concept SearchGraph = 
  requires(T&& g)
  {
      { g.for_each_edge(VertexID{}, [](VertexID, const EdgeProperties&) {}) };
      { g.vertex_count() };
      { g.vertex(VertexID{}) };
  };
```
					</textarea>
				</section>


				<!-- slide -->
				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|1-5|10-11|13-14|16-18]
struct VertexProperties { double x; double y; /* ... */ };

struct EdgeProperties { bool valid; EdgeWeight weight; /* ... */ };

using Edge = std::pair<VertexID, EdgeProperties>;

class Graph
{
public:
  /// Returns vertex properties for a given vertex
  const VertexProperties& vertex(VertexID q) const;

  /// Returns the total number of vertices
  std::size_t vertex_count() const;

  /// Invokes a visitor on each edge of "q"
  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const;
};
```
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3 style="color: white">Something to hold search state!</h3>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold search state!</h3>
						<lu align="left">
							<li>A container to hold visited nodes (and their relationships)</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold search state!</h3>
						<lu align="left">
							<li>A container to hold visited nodes (and their relationships)</li>
							<li>A min-sorted priority queue of vertices</li>
						</lu>
					</section>
				</section>


				<!-- slide -->
				<section data-auto-animate data-markdown>
					 <textarea data-template>
## What do we need?
```c++
template <typename T>
concept SearchContext = 
  requires(T&& ctx)
  {
      { ctx.is_queue_not_empty() };
      { ctx.is_visited(VertexID{}) };
      { ctx.is_terminal(VertexID{}) };
      { ctx.mark_visited(VertexID{}, VertexID{}) };
      { ctx.enqueue(VertexID{}, VertexID{}, EdgeWeight{}) };
      { ctx.dequeue() };
      { ctx.predecessor(VertexID{}) };
  };
```
					</textarea>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|6-8|9-10|11-12|13-14|15-16|17-18|19-20|21-22]
struct Transition { VertexID from; VertexID to; EdgeWeight weight; };

class TerminateAtGoal
{
public:
  /// Resets internal state; sets start and goal vertices
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g);
  /// Returns true if there are still elements in the queue
  bool is_queue_not_empty() const;
  /// Returns true if a vertex has been visited
  bool is_visited(VertexID q) const;
  /// Returns true if a vertex is the goal
  bool is_terminal(VertexID q) const;
  /// Sets a vertex as visited
  void mark_visited(VertexID from, VertexID to);
  /// Returns the predecessor of a vertex
  VertexID predecessor(VertexID q) const;
  /// Get vertex from queue next best distance
  Transition dequeue();
  /// Adds vertex to queue with a distance
  void enqueue(VertexID from, VertexID to, EdgeWeight w);
};
```
					</textarea>
				</section>





				<!------------------ IMPLEMENTATION 0 ------------------>
				<section>
					<h1>Implementation 0</h1>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|1-2|4-11|7|9|12|14|16-27|23-26]
#include <map>
// map, multimap

class Graph
{
private:
  std::map<VertexID, VertexProperties> vertices_;

  std::multimap<VertexID, Edge> adjacencies_;

public:
  const VertexProperties& vertex(VertexID q) const { return vertices_.at(q); }

  std::size_t vertex_count() const { return vertices_.size(); }

  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const
  {
    const auto [first, last] = adjacencies_.equal_range(q);
    std::for_each(
      first,
      last,
      [visitor](const auto& parent_and_edge) mutable
      {
        std::apply(visitor, parent_and_edge.second);
      });
  }
};
```
					</textarea>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|1-4|6-15|9|11|13|16-27|29|31|33|35|37-47|49-54|56-63]
#include <functional>
#include <queue>
#include <map>
#include <vector>

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::map<VertexID, VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.clear();
    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_.emplace(to, from); }

  VertexID predecessor(VertexID q) const
  {
    if (const auto itr = visited_.find(q); itr == visited_.end())
    {
      return q;
    }
    else
    {
      return itr->second;
    }
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```
					</textarea>
				</section>
				<!------------------ IMPLEMENTATION 0 ------------------>





				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

- Run searches over our example warehouse graph
						</textarea>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

- Run searches over our example warehouse graph
- Select some percentage of all vertices, N
						</textarea>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

- Run searches over our example warehouse graph
- Select some percentage of all vertices, N
- Run N^2 searches between all vertices as start/goal pairs
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h3 style="color: white">Run under cachegrind</h3>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

### Run under cachegrind

```bash
> valgrind --tool=cachegrind --cache-sim=yes ./run_demo ...
```	
						</textarea>
					</section>
					<section data-auto-animate data-markdown>
						 <textarea data-template>
## Measuring cache performance
```text [|4-8|10-14]
==131579== Cachegrind, a cache and branch-prediction profiler
==131579== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
…
==131579== I   refs:      20,721,901,316
==131579== I1  misses:             4,744
==131579== LLi misses:             3,909
==131579== I1  miss rate:           0.00%
==131579== LLi miss rate:           0.00%
==131579== 
==131579== D   refs:       7,762,293,651  (6,285,352,547 rd   + 1,476,941,104 wr)
==131579== D1  misses:       615,102,179  (  604,712,021 rd   +    10,390,158 wr)
==131579== LLd misses:         3,336,167  (    2,266,653 rd   +     1,069,514 wr)
==131579== D1  miss rate:            7.9% (          9.6%     +           0.7%  )
==131579== LLd miss rate:            0.0% (          0.0%     +           0.1%  )
==131579== 
==131579== LL refs:          615,106,923  (  604,716,765 rd   +    10,390,158 wr)
==131579== LL misses:          3,340,076  (    2,270,562 rd   +     1,069,514 wr)
==131579== LL miss rate:             0.0% (          0.0%     +           0.1%  )

```
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h3 style="color: white">Run under hotspot (perf)</h3>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

### Run under hotspot (perf)

```bash
> sudo apt install linux-tools-6.2.0-33-generic

> sudo hotspot
```	
						</textarea>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

### Instructions

![me](slides/v0_hotspot_instructions.png)
						</textarea>
					</section>
					<section data-auto-animate data-markdown>
						<textarea data-template>
## Measuring cache performance

### Cache-misses

![me](slides/v0_hotspot_cache_misses.png)
						</textarea>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Spatial Locality</h3>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Spatial Locality</h3>
						<lu align="left">
							<li>Data that we access should be close together in memory.</li>
						</lu>
					</section>

					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Temporal Locality</h3>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Temporal Locality</h3>
						<lu align="left">
							<li>Maximize the number of times we access data in the cache.</li>
						</lu>
					</section>

					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Rules of thumb</h3>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Rules of thumb</h3>
						<lu align="left">
							<li>Access data sequentially.</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Rules of thumb</h3>
						<lu align="left">
							<li>Access data sequentially.</li>
							<li>Reduce indirections through pointers.</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Rules of thumb</h3>
						<lu align="left">
							<li>Access data sequentially.</li>
							<li>Reduce indirections through pointers.</li>
							<li>Do not jump around memory frequently.</li>
						</lu>
					</section>

					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Does out first implementation do any of these things?</h3>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Does out first implementation do any of these things?</h3>
						<h1 style="color: white">No not really.</h3>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>std::multimap</h2>
					</section>
					<section data-auto-animate>
						<h2>std::multimap</h2>
						<div class="r-stack">
							<img src="slides/v0_multimap_rep.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>std::multimap</h2>
						<div class="r-stack">
							<img src="slides/v0_multimap_actual.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>std::multimap</h2>
						<div class="r-stack">
							<img src="slides/v0_multimap_mem.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>std::multimap</h2>
						<div class="r-stack">
							<img src="slides/v0_multimap_equal_range.png">
						</div>
					</section>
				</section>



				<!------------------ IMPLEMENTATION 1 ------------------>
				<section>
					<h1>Implementation 1</h1>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|1-2|4-11|7|9|12|14|16-27|23-26]
#include <unordered_map>
// unordered_map, unordered_multimap

class Graph
{
private:
  std::unordered_map<VertexID, VertexProperties> vertices_;

  std::unordered_multimap<VertexID, Edge> adjacencies_;

public:
  const VertexProperties& vertex(VertexID q) const { return vertices_.at(q); }

  std::size_t vertex_count() const { return vertices_.size(); }

  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const
  {
    const auto [first, last] = adjacencies_.equal_range(q);
    std::for_each(
      first,
      last,
      [visitor](const auto& parent_and_edge) mutable
      {
        std::apply(visitor, parent_and_edge.second);
      });
  }
};
```
					</textarea>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|1-4|6-15|9|11|13|16-27|29|31|33|35|37-47|49-54|56-63]
#include <functional>
#include <queue>
#include <unordered_map>
#include <vector>

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::unordered_map<VertexID, VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.clear();
    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_.emplace(to, from); }

  VertexID predecessor(VertexID q) const
  {
    if (const auto itr = visited_.find(q); itr == visited_.end())
    {
      return q;
    }
    else
    {
      return itr->second;
    }
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```
					</textarea>
				</section>
				<!------------------ IMPLEMENTATION 1 ------------------>


				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown>
						 <textarea data-template>
## Measuring cache performance
```text [|4-8|10-14]
==132432== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==132432== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==132432== I   refs:      11,574,779,293
==132432== I1  misses:             3,778
==132432== LLi misses:             3,700
==132432== I1  miss rate:           0.00%
==132432== LLi miss rate:           0.00%
==132432== 
==132432== D   refs:       4,525,697,105  (2,946,691,588 rd   + 1,579,005,517 wr)
==132432== D1  misses:       109,346,888  (  103,780,511 rd   +     5,566,377 wr)
==132432== LLd misses:         3,231,301  (    2,205,723 rd   +     1,025,578 wr)
==132432== D1  miss rate:            2.4% (          3.5%     +           0.4%  )
==132432== LLd miss rate:            0.1% (          0.1%     +           0.1%  )
==132432== 
==132432== LL refs:          109,350,666  (  103,784,289 rd   +     5,566,377 wr)
==132432== LL misses:          3,235,001  (    2,209,423 rd   +     1,025,578 wr)
==132432== LL miss rate:             0.0% (          0.0%     +           0.1%  )

```
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v0_v1_cmi.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v1_multimap_rep.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v1_multimap_actual.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v1_multimap_mem.png">
						</div>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v1_multimap_equal_range.png">
						</div>
					</section>
				</section>


				<!------------------ IMPLEMENTATION 1.5 ----------------->
				<section>
					<h1>Implementation 1 + reserve buckets</h1>
				</section>

				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [20-21]
#include <functional>
#include <queue>
#include <unordered_map>
#include <vector>

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::unordered_map<VertexID, VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.clear();
    visited_.reserve(graph.vertex_count());
    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_.emplace(to, from); }

  VertexID predecessor(VertexID q) const
  {
    if (const auto itr = visited_.find(q); itr == visited_.end())
    {
      return q;
    }
    else
    {
      return itr->second;
    }
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown>
						 <textarea data-template>
## Measuring cache performance
```text [|4-8|10-14]
==133350== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==133350== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==133350== I   refs:      11,574,779,289
==133350== I1  misses:             6,017
==133350== LLi misses:             3,935
==133350== I1  miss rate:           0.00%
==133350== LLi miss rate:           0.00%
==133350== 
==133350== D   refs:       4,525,697,103  (2,946,691,588 rd   + 1,579,005,515 wr)
==133350== D1  misses:        90,999,080  (   85,486,438 rd   +     5,512,642 wr)
==133350== LLd misses:         3,231,302  (    2,205,724 rd   +     1,025,578 wr)
==133350== D1  miss rate:            2.0% (          2.9%     +           0.3%  )
==133350== LLd miss rate:            0.1% (          0.1%     +           0.1%  )
==133350== 
==133350== LL refs:           91,005,097  (   85,492,455 rd   +     5,512,642 wr)
==133350== LL misses:          3,235,237  (    2,209,659 rd   +     1,025,578 wr)
==133350== LL miss rate:             0.0% (          0.0%     +           0.1%  )

```
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v0_v1_v1res_cmi.png">
						</div>
					</section>
				</section>
				<!------------------ IMPLEMENTATION 1.5 ----------------->





				<!------------------ IMPLEMENTATION 2 ------------------>
				<section>
					<h1>Implementation 2</h1>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|]
#include <vector>

class Graph
{
private:
  std::vector<VertexProperties> vertices_;

  std::vector<std::vector<Edge>> adjacencies_;

public:
  const VertexProperties& vertex(VertexID q) const { return vertices_[q]; }

  std::size_t vertex_count() const { return vertices_.size(); }

  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const
  {
    std::for_each(
      adjacencies_[q].begin(),
      adjacencies_[q].end(),
      [q, visitor](const auto& child_and_edge_weight) mutable
      {
        const auto& [succ, edge_weight] = child_and_edge_weight;
        visitor(succ, edge_weight);
      });
  }
};
```
					</textarea>
				</section>


				<section data-auto-animate data-markdown>
					 <textarea data-template>
```c++ [|]
#include <functional>
#include <queue>
#include <vector>

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::vector<VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.resize(graph.vertex_count());
    visited_.assign(graph.vertex_count(), graph.vertex_count());

    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_[to] = from; }

  VertexID predecessor(VertexID q) const
  {
    return visited_[q];
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```
					</textarea>
				</section>
				<!------------------ IMPLEMENTATION 2 ------------------>


				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown>
						 <textarea data-template>
## Measuring cache performance
```text [|4-8|10-14]
==135295== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==135295== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==135295== I   refs:      6,919,489,865
==135295== I1  misses:            6,058
==135295== LLi misses:            3,887
==135295== I1  miss rate:          0.00%
==135295== LLi miss rate:          0.00%
==135295== 
==135295== D   refs:      2,516,605,627  (1,700,911,074 rd   + 815,694,553 wr)
==135295== D1  misses:       43,267,746  (   40,602,167 rd   +   2,665,579 wr)
==135295== LLd misses:        3,054,946  (    2,183,488 rd   +     871,458 wr)
==135295== D1  miss rate:           1.7% (          2.4%     +         0.3%  )
==135295== LLd miss rate:           0.1% (          0.1%     +         0.1%  )
==135295== 
==135295== LL refs:          43,273,804  (   40,608,225 rd   +   2,665,579 wr)
==135295== LL misses:         3,058,833  (    2,187,375 rd   +     871,458 wr)
==135295== LL miss rate:            0.0% (          0.0%     +         0.1%  )

```
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v0_v1_v1res_v2_cmi.png">
						</div>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can re-order vertices and their adjacencies with std::sort</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can re-order vertices and their adjacencies with std::sort</li>
							<li>If we can find a single sort which works for many start and goal pairs, then its practical to do</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can re-order vertices and their adjacencies with std::sort</li>
							<li>If we can find a single sort which works for many start and goal pairs, then its practical to do</li>
							<li>We know that vertices which are close in space are more likely to be neighbors</li>
						</lu>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>If we can find a single sort which works for many start and goal pairs, then its practical to do</li>
							<li>We know that vertices which are close in space are more likely to be neighbors</li>
							<li>We can order vertex adjacency data in memory such that the probability of accessing nearby vertex data in the same cache line is higher</li>
						</lu>
					</section>
					<section data-auto-animate data-markdown>
						 <textarea data-template>
## Impact of ordering of the graph
```c++ [|5-13|12|15]
std::vector<std::size_t> remapping;
remapping.resize(graph.vertex_count());
std::iota(remapping.begin(), remapping.end(), 0);

std::sort(
  remapping.begin(),
  remapping.end(),
  [&graph](VertexID lhs, VertexID rhs) -> bool
  {
    const auto& lv = graph.vertex(lhs);
    const auto& rv = graph.vertex(rhs);
    return ((lv.x * lv.x) + (lv.y + lv.y)) < ((rv.x * rv.x) + (rv.y + rv.y));
  });

graph.remap(remapping);
```
						</textarea>
					</section>
				</section>



				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown>
						 <textarea data-template>
## Measuring cache performance
```text [|4-8|10-14]
==135709== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==135709== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==135709== I   refs:      6,944,720,850
==135709== I1  misses:            6,115
==135709== LLi misses:            3,906
==135709== I1  miss rate:          0.00%
==135709== LLi miss rate:          0.00%
==135709== 
==135709== D   refs:      2,526,314,915  (1,707,693,876 rd   + 818,621,039 wr)
==135709== D1  misses:       28,448,985  (   25,770,132 rd   +   2,678,853 wr)
==135709== LLd misses:        3,054,896  (    2,183,489 rd   +     871,407 wr)
==135709== D1  miss rate:           1.1% (          1.5%     +         0.3%  )
==135709== LLd miss rate:           0.1% (          0.1%     +         0.1%  )
==135709== 
==135709== LL refs:          28,455,100  (   25,776,247 rd   +   2,678,853 wr)
==135709== LL misses:         3,058,802  (    2,187,395 rd   +     871,407 wr)
==135709== LL miss rate:            0.0% (          0.0%     +         0.1%  )

```
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/v0_v1_v1res_v2_v2sorted_cmi.png">
						</div>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
							</lu>
						</div>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
							</lu>
						</div>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
								<li>We can use the STL to effectively implement a SBP algorithm</li>
							</lu>
						</div>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
								<li>We can use the STL to effectively implement a SBP algorithm</li>
								<li>Memory ordering has a large impact (but we do not need to jump straight to writing allocators)</li>
							</lu>
						</div>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
								<li>We can use the STL to effectively implement a SBP algorithm</li>
								<li>Memory ordering has a large impact (but we do not need to jump straight to writing allocators)</li>
								<li>Cache pipelines are complex, so measurement is our best bet!</li>
							</lu>
						</div>
					</section>


				<!-- slide -->
				<section>
					<h3 style="color: white">https://github.com/briancairl/cppcon2023</h3>
				<second>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
