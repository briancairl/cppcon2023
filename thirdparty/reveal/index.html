<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>CppCon 2023</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/custom.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<!-- slide -->
				<section data-background="slides/cppcontitle.png">
					<aside class="notes">
						<p>Hello everyone.</p>
						<p>My name is Brian Cairl and I am presenting: Cache-friendly design in robot path planning</p>
					</aside>
				</section>


				<!-- slide -->
				<section data-transition="fade">
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<ul align="left">
								  <li>This is my first time at CppCon!</li>
								  <li>Regularly program in C++ for robotics</li>
								</ul>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>This is my first live cppcon.</p>
							<p>Last year I attended remotely, so I'm excited to be here in person.</p>
							<p>I've programmed in C++ on a daily basis as a robotics software developer for the past 7 years.</p>
							<p>Though, I've been a C++ programmer since 2012 or so.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<div align="left">
									<h4 style="color: white">I've worked at a few robotics startups:</h4>
									<ul align="left">
										<li>Fetch Robotics (2016-2022)</li>
										<li>Thirdwave.ai (2022-2023)</li>
										<li>Tortuga AgTech (present)</li>
									</ul>
								</div>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>I've worked on mostly mobile robot systems which operate in warehouses.</p>
							<p>But as of a few months ago I started working at a robot agriculture company called Tortuga AgTech, so first time in 7 years, I'm finely getting some fresh air.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<div align="left">
									<h4 style="color: white">Mostly doing:</h4>
									<ul align="left">
										<li>General architecture and frameworks around robot systems</li>
										<li>Navigation and perception systems</li>
									</ul>
								</div>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>I typically write broader robot systems code.</p>
							<p>But I've focused on mostly algorithms and architecture around robot navigation and perception.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<div class="row">
							<div class="column">
								<h2>About Me</h2>
								<div align="left">
									<h4 style="color: white">Other interests:</h4>
									<ul>
										<li>Game engines</li>
										<li>Meta-programming</li>
									</ul>
								</div>
							</div>
							<div class="column">
								<div class="r-stack">
									<img src="slides/me.jpeg">
								</div>
							</div>
						</div>
						<aside class="notes">
							<p>Besides coding in C++ for work, I also code for fun.</p>
							<p>In 1000 years I'll finish a game engine.</p>
							<p>And to procrastinate I tool around template meta-programming nonsense.</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2 style="color: white">Robot path planning</h2>
						<aside class="notes">
							<p>To parse the title of this talk backwards, lets start with: robot path planning</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Robot path planning</h2>
						<h2 style="color: white">Path planning</h2>
						<aside class="notes">
							<p>Or more generally just: path planning</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Path planning</h2>
						<ul>
						How an (autonomous) agent figures out how to get from one location to another before actually moving.
						</ul>
						<div class="r-stack">
							<img src="slides/robot_path_planning.png" width="500" height="auto">
						</div>
						<aside class="notes">
							<p>Refers to how an (autonomous) agent figures out how to get from one location to another before actually moving.</p>
							<p>Within robotics this extends to a variety of sub-domains.</p>
							<p>We plan paths for 9-jointed robot arm end effectors moving car parts on and off of conveyor belts.</p>
							<p>Just as we plan paths for our robot vacuum cleaners to make their way through our houses.</p>
							<p>But the concept of path planning, obviously, extends outside the domain of robotics.</p>
							<p>Many of the same planning algorithms used in robots finds thier place in video games; online map services; even computer network packet routing</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<aside class="notes">
							<p>So now we're onto the the slightly more philosophical, programming-focused part of the title: cache friendly design</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
						Program design focused on optimizing code by avoiding pathological affects of memory access through the caching structure of a computer system.
						</ul>
						<aside class="notes">
							<p>Cache friendly design refers to: program design guided by attempts to prevent the CPU from slowing down while accessing data in memory</p>
							<p>And because most CPUs we actually use are faster than our memory hardware: we are accessing that data through some complex pipeline of connected cache levels which sit between the CPU and main memory.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div>
							<blockquote>Modern memory pipelines are so complex you are basically optimizing for the cache</blockquote>
							- Random person on StackOverflow
						</div>
						<aside class="notes">
							<p>And these are indeed complex and important</p>
							<p>This guy said so</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/robot_target_system.png">
						</div>
						<aside class="notes">
							<p>Robots have brains made of computers</p>
							<p>Most of the robots I've ever worked on use the same CPU hardware that sits in most high-end laptops</p>
							<p>So at least in the domain of mobile robotics I'm covering a lot of ground by claiming that, as far as hardware is concerned: the CPUs we use to execute high-level software like path-planners, generally isn't all that exotic</p>
							<p>And we are optimizing for the same or similar targets that most people working in a non-embedded systems land are optimizing for.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/robot_program_architecture.png">
						</div>
						<aside class="notes">
							<p>A typical robot system might have some high-level architecture that looks like this</p>
							<p>Where we have a bunch of non-hard-real-time applications running together in user space</p>
							<p>And they are talking to each other with some inter-process message-passing protocol</p>
							<p>A popular choice for this sort of structure is ROS (the Robot operating system), which is not actually an OS</p>
							<p>And they are interfaced with some lower level, probably -- but not definitely -- real-time systems for actually getting the robot to move and; reacting safely and deterministically around critical things like human passersby</p>
							<p>At least we hope</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/cache.png">
						</div>
						<aside class="notes">
							<p>All that to say: at least for the most brain-like part of the system, we are running on a consumer-grade CPU</p>
							<p>With a consumer-grade memory pipeline</p>
							<p>That is probably organized something like what this probably-familiar diagram aims to show where: we have small, but fast caches close to our cores</p>
							<p>And some large, slower cache interfaced with main memory</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
						</ul>
						<aside class="notes">
							<p>Any time one of our high-level programs reads or writes data to memory, it is doing so through the cache</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>	
						</ul>
						<aside class="notes">
							<p>Any time one of our high-level programs executes instructions, it is first reading them from the cache</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>	
							<li>Besides the data the CPU needs now, the cache is populated with data likely to be accessed soon (pre-fetching)</li>
						</ul>
						<aside class="notes">
							<p>In order to reduce the amount of times that we need to reach into slow, main memory, the cache is populated with possibly relevant data besides the data the program is currently accessing</p>
							<p>This is called pre-fetching</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>
							<li>Besides the data the CPU needs now, the cache is populated with data likely to be accessed soon (pre-fetching)</li>
							<li>Programs may attempt to access data that is not in the cache or has been removed (evicted) from the cache</li>
						</ul>
						<aside class="notes">
							<p>When things are already in the L1 cache, the CPU is happy, because it doesn't need to wait for data that it needs to run the next set of instructions</p>
							<p>But sometimes data a program might need has already been removed from the cache, or it wasn't in the cache to begin with</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Accessing data means accessing memory through the cache</li>
							<li>Running instructions means accessing memory through the cache</li>
							<li>Besides the data the CPU needs now, the cache is populated with data likely to be accessed soon (pre-fetching)</li>
							<li>Programs may attempt to access data that is not in the cache or has been removed (evicted) from the cache</li>
							<li>If a program attempts to access something that not in the cache, this is a <em style="color:red">cache miss</em></li>	
						</ul>
						<aside class="notes">
							<p>This is called a cache miss</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<div class="r-stack">
							<img src="slides/cache_miss.png">
						</div>
						<aside class="notes">
							<p>Cache misses can happen at each level of the cache</p>
							<p>If data isn't available in an L1 cache line, the L2 cache is checked, and so on and so forth</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
						</ul>
						<aside class="notes">
							<p>The cache exists to bridge the gap between the difference in speed between CPU and memory hardware</p>
							<p>And on the happy path, this obviously works</p>
							<p>But cache misses potentially add extra latency to program execution</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
						</ul>
						<aside class="notes">
							<p>Accessing data at each cache level, from L1 down to main memory, is considerably slower than the previous level</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
							<li>CPUs can run out of things to do while waiting for the the next cache line from memory</li>	
						</ul>
						<aside class="notes">
							<p>So a bad cache miss where the data isn't in any cache can cause the CPU to wait</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
							<li>CPUs can run out of things to do while waiting for the the next cache line from memory</li>
							<li>This is called a CPU stall</li>
						</ul>
						<aside class="notes">
							<p>This is referred to as a CPU stall</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<ul align="left">
							<li>Cache misses introduce latency</li>	
							<li>Access through each level is slower than the previous level</li>	
							<li>CPUs can run out of things to do while waiting for the the next cache line from memory</li>
							<li>This is called a CPU stall</li>
							<li>There are mitigations, e.g. out-of-order execution</li>
						</ul>
						<aside class="notes">
							<p>There are mitigations beyond our control, such as out-of-order execution, that allow a CPU can run instructions while waiting on a cache-miss to be satisfied</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<h3 style="color: white">Optimizes how a program uses data already in the cache.</h3>
						<aside class="notes">
							<p>But we can certainly attempt to write code in such a way that makes the best use of the cache pipeline</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Cache-friendly design</h2>
						<h1 style="color: white">Minimizes cache misses!</h1>
						<aside class="notes">
							<p>And in order to do that, we will have to minimize the number of cache-misses that occur during the execution of our programs</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<aside class="notes">
							<p>With all that background in mind: some high level goals of this talk</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
						</ul>
						<aside class="notes">
							<p>I'd like to show how to implement some path-planning code that could be used on a robot system, video game, etc. that is performant, and, for the sake of this talk, specifically cache-performant</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
							<li>Do so using the STL</li>
						</ul>
						<aside class="notes">
							<p>I want to show that we can do so using facilities that are readily available to C++ programmers out of the box</p>
							<p>Namely, with things that are available in the C++ STL</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
							<li>Do so using the STL</li>
							<li>Try to keep our choices simple</li>
						</ul>
						<aside class="notes">
							<p>And I will show that we can keep our choices relative simple</p>
							<p>We can do alot have without having to do things like touch custom memory allocators</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>High level goals of this talk</h2>
						<ul>
							<li>Implement cache-friendly path planning code in C++</li>
							<li>Do so using the STL</li>
							<li>Try to keep our choices simple</li>
							<li>Measure</li>
						</ul>
						<aside class="notes">
							<p>I will also attempt to show how we can make these choices guided by measurement</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<aside class="notes">
							<p>To kick off an implementation journey, lets focus on a motivating problem</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3 style="color: white">Robots typically need to move.</h3>
						<aside class="notes">
							<p>We want our robots to move</p>
							<p>A noble goal</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3 style="color: white">Mobile Robot Systems</h3>
						<aside class="notes">
							<p>Lets focus on mobile robot systems</p>
							<p>Mobile means they definitely should be moving</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>Warehouse robots: forklifts, pallet-movers, etc.</li>
						</ul>
						<aside class="notes">
							<p>Lets focus even more specifically on robots that can pretend they live in a 2D world</p>
							<p>Like autonomous forklifts moving pallets around</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>Warehouse robots: forklifts, pallet-movers, etc.</li>
							<li>Home robots: vacuums, etc.</li>
						</ul>
						<aside class="notes">
							<p>Home robots moving around your house, moving from room to room optimally to conserve battery and maximize the amount of cat hair they pick up</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>Warehouse robots: forklifts, pallet-movers, etc.</li>
							<li>Home robots: vacuums, etc.</li>
							<li>Outdoor robots: tractors, berry-pickers, delivery platforms, etc.</li>
						</ul>
						<aside class="notes">
							<p>Or even outdoor robots like robot tractors; berry-pickers; or robot delivery systems</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>These robots operate in highly dynamic environments.</li>
						</ul>
						<aside class="notes">
							<p>Even though these robots hallucinate a low dimensional space, moving around is a hard problem because the environment they work in is chaotic</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>These robots operate in highly dynamic environments.</li>
							<li>Robots might be informed about blockages local observation or some centralized coordination system.</li>
						</ul>
						<aside class="notes">
							<p>They might experience blockages while driving that they directly observe or are informed about by some coordinating central system or from information shared from other robots</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<h3>Mobile Robot Systems</h3>
						<ul>
							<li>These robots operate in highly dynamic environments.</li>
							<li>Robots might be informed about blockages local observation or some centralized coordination system.</li>
							<li>A robot might need to figure out how to move multiple times while making its way to a goal.</li>
						</ul>
						<aside class="notes">
							<p>So they likely need to be reactive with how they come up with new plans towards a goal they are moving to.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<div class="r-stack">
							<img src="slides/robot_navigation_system.png">
						</div>
						<aside class="notes">
							<p>A hypothetical state machine for a navigating robot system might look like this.</p>
							<p>A robot is initialized with some information about its environment, typically in the form of a map and/or a graph.</p>
							<p>It gets commands to go somewhere else.</p>
							<p>It comes up with some high-level reference plan all the way to its goal</p>
							<p>We will refer to this as a global plan</p>
							<p>And then it follows that global plan as a reference while it uses some other lower-level planner/optimizer/controller to make adjustments around obstacles</p>
							<p>When the local control systems can't figure out how to react, the system will likely drop back to the global planner for a new global plan</p>
							<p>And so on and so forth until the robot gets to its intended goal location</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
						</ul>
						<aside class="notes">
							<p>Coming up with these global plans is a relatively infrequent occurrence given everything else going on in the system at a given time</p>
							<p>But when they happen, its typically resource intensive</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
						</ul>
						<aside class="notes">
							<p>This only gets worse for larger planning state-spaces</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
							<li>We are concerned about computation time as it affects overall system latency stack-up.</li>
						</ul>
						<aside class="notes">
							<p>So: we are concerned with minimizing the amount of time it takes to come up with a global plan as it factors into how reactive our overall system is</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
							<li>We are concerned about computation time as it affects overall system latency stack-up.</li>
							<li>When we tell a robot to do something, we want it moving quickly.</li>
						</ul>
						<aside class="notes">
							<p>We typically want our robots to look intelligent, so they have to be snappy about re-routing</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The motivating problem</h2>
						<ul>
							<li>Figuring out how to move is computationally expensive.</li>
							<li>This only gets worse as the state-space of the problem gets bigger.</li>
							<li>We are concerned about computation time as it affects overall system latency stack-up.</li>
							<li>When we tell a robot to do something, we want it moving quickly.</li>
							<li style="color: cyan">Fast planning algorithms == less time robots are sitting around.</li>
						</ul>
						<aside class="notes">
							<p>And someone is paying for these robots to do things so: chop-chop, time is money, the robot's cant be sitting around for long</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						</ul>
						<aside class="notes">
							<p>Lets look at a typical environment that our robots might be moving through</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						<h3 style="color: white">Robots operating in a very big warehouse.</h3>
						<aside class="notes">
							<p>An easy example that we can't seem to get away from these days is some mega-warehouse butlered by mobile robots</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility.png" width="1500", height="auto">
						</div>
						<aside class="notes">
							<p>Here is a top-down map of a make-believe distribution facility</p>
							<p>Its probably quite hard to see, but it has a bunch of repeated areas with shelves probably holding beans or protein powder or whatever folks order from a mega-corp</p>
							<p>And at the bottom and top are some areas where a robot might take said product to be packaged for shipping</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility.png" width="1500", height="auto">
						</div>
						<div align="center">
							This is a map of a make-believe distribution facility that is:
							<lu align>
								<li>0.8 km long</li>
								<li>0.4 km deep</li>
							</lu>
						</div>
						<aside class="notes">
							<p>This facility is fairly big, but not the biggest</p>
							<p>Its 0.8 by 0.4 km where each pixel of this image represents 5cm of space</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility_graph.png">
						</div>
						<lu>This is a spatial graph extracted from the map.</lu>
						<aside class="notes">
							<p>And here is a spatial graph I extracted from the map using a fancy little python script</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility_graph_zoomed.png">
						</div>
						<lu>This is a spatial graph informs the robot about good "roadways" to travel along.</lu>
						<aside class="notes">
							<p>So zooming in a bit we can see things a bit better</p>
							<p>The graph effectively establishes some nice, central pathways between aisles and shelves that the robot can plan along</p>
							<p>These are not unlike a static network of roadways used by GoogleMaps</p>
							<p>Or pre-scripted pathways for characters to walk in a video game</p>
							<p>Lets assume that the paths we extracted from this graph are all kinematically sane and traversable by our robot system so that the robot doesn't plan itself into an area it can't physical traverse</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>The operating environment</h2>
						<div class="r-stack">
							<img src="slides/fake_distribution_facility_graph_path.png">
						</div>
						<lu>A robot could used this graph to make global plans using this graph to get around the warehouse.</lu>
						<aside class="notes">
							<p>To get from place to place, a robot would compute a global plan from its current location to some other specific location within the map</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-transition="fade">
						<h2>Why not just pre-compute all paths?</h2>
						<div class="r-stack">
							<img src="slides/no_precompute.png">
						</div>
						<aside class="notes">
							<p>Just to get this out of the way: why not just also pre-compute all the possible paths the robot can take rather than run an on-line search?</p>
							<p>I mean, probably the best we can do in terms of memory performance is just load sequential blocks of data from memory</p>
							<p>You know, just completely bypass the need for some complicated algorithm in the first place</p>
							<p>And if it was simply a matter of traveling from vertex to vertex, we probably could probably get away with this, even for relatively large graphs</p>
						</aside>
					</section>
					<section data-transition="fade">
						<h2>Why not just pre-compute all paths?</h2>
						<div class="r-stack">
							<img src="slides/no_precompute_2.png">
						</div>
						<aside class="notes">
							<p>But, as I mentioned, the environment is dynamic. Edges become inaccessible due to blockages we can't know about ahead time</p>
							<p>And this changes the topology of the graph</p>
							<p>Pre-computing every possible plan for every possible blockage would lead to a combinatorial explosion of a lookup table</p>
							<p>So, practically speaking, we will need to run some kind of on-line search unless our graph is very small</p>
							<p>As a sort of throw-away remark, we could still use precomputed paths as a heuristic to speed up our algorithms, but lets put that aside</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What do we do?</h2>
						<aside class="notes">
							<p>So what do we do?</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2 style="color: white">Shortest-path search algorithms</h2>
						<aside class="notes">
							<p>We'll employ a shortest path search algorithm</p>
						</aside>
					</section>
					<section data-auto-animate data-visibility="hidden">
						<h2>Shortest-path search algorithms</h2>
						<h3 style="color: white">Sampling-based</h3>
						<aside class="notes">
							<p>Some common ones used in the field are sampling based, where we basically make structured-random guesses to fill space based on the robots kinematics or other constraints.</p>
						</aside>
					</section>
					<section data-auto-animate data-visibility="hidden">
						<h2>Shortest-path search algorithms</h2>
						<h3>Sampling-based</h3>
						<lu align="left">
							<li>Rapidly-exploring random trees (RRT)</li>
						</lu>
						<aside class="notes">
							<p>Just to name some: these include RRT: Rapidly-exploring random trees</p>
						</aside>
					</section>
					<section data-auto-animate data-visibility="hidden">
						<h2>Shortest-path search algorithms</h2>
						<h3>Sampling-based</h3>
						<lu align="left">
							<li>Rapidly-exploring random trees (RRT)</li>
							<li>Probabilistic road maps (PRM)</li>
						</lu>
						<aside class="notes">
							<p>These include PRM: Probabilistic road maps</p>
						</aside>
					</section>


					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3 style="color: white">Search-based</h3>
						<aside class="notes">
							<p>Because we are coming to the table with some prior graph, the flavor of search algorithm we will focus on is the search based variety.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
						</lu>
						<aside class="notes">
							<p>The canonical example, in my opinion, being Dijkstra's algorithm.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
						</lu>
						<aside class="notes">
							<p>We also have, A*, which is effectively a superset-form of Dijkstra's.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
							<li>Lifelong planning A* (LPA*)</li>
						</lu>
						<aside class="notes">
							<p>LPA*</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
							<li>Lifelong planning A* (LPA*)</li>
							<li>D*</li>
						</lu>
						<aside class="notes">
							<p>D*</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li>Dijkstra's algorithm</li>
							<li>A*</li>
							<li>Lifelong planning A* (LPA*)</li>
							<li>D*</li>
						</lu>
						<aside class="notes">
							<p>and so on</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Shortest-path search algorithms</h2>
						<h3>Search-based</h3>
						<lu align="left">
							<li style="color: cyan;">Dijkstra's algorithm</li>
							<li style="color: darkcyan;">A*</li>
							<li style="color: grey;">Lifelong planning A* (LPA*)</li>
							<li style="color: grey;">D*</li>
							<li style="color: grey;">etc.</li>
						</lu>
						<aside class="notes">
							<p>From this set, we will focus on is Dijkstra's algorithm.</p>
							<p>The reason for this is that, anecdotally, is that it is a common go to for planning system where you can know the graph you are planning over ahead of time.</p>
							<p>So it covers a lot of practical application ground.</p>
							<p>Its a relatively simple algorithm to show as C++.</p>
							<p>And it can definitely be implemented with the STL pretty effectively.</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<h2>Dijkstra's algorithm (early stopping)</h2>
					<div class="r-stack">
						<img src="slides/dijkstras_fast.gif" height="400", width="400">
					</div>
					<aside class="notes">
						<p>So here is a semi famous wikipedia graphic for Dijkstra's</p>
						<p>We are looking at spatial graph is embedded upon some kind of uniform lattice structure, like a grid</p>
					</aside>
				</section>


				<!-- slide -->
				<section data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
## Dijkstra's algorithm (early stopping)

```r [|1|3-6|8|9-10|11-13|15-16|17-19|21]
function Dijkstras(Graph, source, target):

  for each vertex v in Graph.Vertices:
    prev[v] ← UNDEFINED                                 # predecessors of v (visited set)
  create (vertex, vertex, dist) priority queue Q        # create a priority queue
  Q.add_with_priority((start, start, 0))                # add start vertex to priority queue

  while Q is not empty:
    (p,u,dist_u) ← Q.extract_min()                      # remove next best vertex (u) and its
                                                        # predecessor (p)
    if prev[u] is not UNDEFINED:                        # if we have visited (u), ignore
      continue
    prev[u] ← p                                         # otherwise, set predecessor of (u)

    if u is target:                                     # end if we are at the target vertex
      break
    for each neighbor v of u with prev[v] is UNDEFINED:
      dist_v ← dist_u + Graph.Edge(u, v)                # compute distance to (v)
      Q.add_with_priority((u, v, dist_v))               # enqueue (v,u,dist)

  return prev[]                                         # return predecessors to recover path
```

Note:
0. And here's the actual algorithm
1. We are implementing a point-to-point version of Dijkstra's so our function will accept some prior graph; and a start and goal vertex within that graph
2. To initialize we are going to create a "visited set" which will be used to memoize which vertices we've evaluated, and how we got there; as well as a priority queue, which will contain the next-best vertices to evaluate based on their total computed distance from the start. We'll put the start vertex in this queue with 0 distance, which will be the first vertex we pop off the queue.
3. The algorithm will proceed until the queue is empty
4. At each step, we will remove the vertex from the queue with the smallest total distance
5. If we've visited that vertex, we'll ignore it; otherwise we'll record it as visited, taking note of how we got there
6. If the vertex is the goal, we will stop the search, since we only care about a path from the start to the goal. In what I think is the more well-known version of Dijkstra's, we search from a start vertex to all other vertices in the graph. If we want optimal paths to all other vertices, we can just omit this line and the search will always run until the queue is empty.
7. Otherwise, we will inspect each connected vertex that we haven't yet visited, and add those vertices to the queue with an updated total distance
7. When we've found the goal, the algorithm will return a set of transitions that we can walk through from the goal to the start. The path we can recover by doing this be the shortest path.
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<h2>A* (early stopping)</h2>
					<div class="r-stack">
						<img src="slides/astar_fast.gif" height="400", width="400">
					</div>
					<aside class="notes">
						<p>And here is another semi famous wikipedia graphic for A*</p>
						<p>We see that, compared to Dijkstra's, the algorithm has to visit less vertices to find a path to the goal</p>
					</aside>
				</section>


				<!-- slide -->
				<section data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
## A* (early stopping)

```r [18]
function Astar(Graph, source, target):

  for each vertex v in Graph.Vertices:
    prev[v] ← UNDEFINED                                 # predecessors of v (visited set)
  create (vertex, vertex, dist) priority queue Q        # create a priority queue
  Q.add_with_priority((start, start, 0))                # add start vertex to priority queue

  while Q is not empty:
    (p,u,dist_u) ← Q.extract_min()                      # remove next best vertex (u) and its
                                                        # predecessor (p)
    if prev[u] is not UNDEFINED:                        # if we have visited (u), ignore
      continue
    prev[u] ← p                                         # otherwise, set predecessor of (u)

    if u is target:                                     # end if we are at the target vertex
      break
    for each neighbor v of u with prev[v] is UNDEFINED:
      dist_v ← dist_u + Graph.Edge(u, v) + H(v)         # compute distance to (v) + heuristic
      Q.add_with_priority((u, v, dist_v))               # enqueue (v,u,dist)

  return prev[]                                         # return predecessors to recover path
```

Note:
- When compared to Dijkstra's we also see that the algorithms are basically the same. The exception to this is how we compute the next total distance for a given vertex
- Here we apply some heuristic function H(v) when summing up the next total distance
- The heuristic function estimates the total cost to the goal, which changes the order in which vertices are evaluated

- So I'm mentioning this much about A* because the first optimization one can make when implementing Dijkstra's is: just use A* and pick a good heuristic
- But here we are interested in optimizing the work the CPU is doing in a somewhat orthogonal domain
- I only mention A* because it is a better go-to for robot path planning, in most cases; but more importantly, any cache-guided optimizations we can make in a Dijkstra's implementation will easily carry over to A*
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<aside class="notes">
							<p>Getting back on topic, lets finally get to some C++</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
						</lu>
						<aside class="notes">
							<p>Here, we want to implement Dijkstras in C++</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
						</lu>
						<aside class="notes">
							<p>We want to use the STL, as-is, as much as we can</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
							<li>Measure cache performance</li>
						</lu>
						<aside class="notes">
							<p>We want to measure the cache-performance of this implementation</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Implementation Goals</h2>
						<lu align="left">
							<li>Implement Dijkstra's algorithm in C++</li>
							<li>Use the STL: containers and other facilities</li>
							<li>Measure cache performance</li>
							<li>Incrementally improve performance with better choices</li>
						</lu>
						<aside class="notes">
							<p>And then we want to make improvements through better choices within the STL, guided by our measurements</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What do we need?</h2>
						<aside class="notes">
							<p>What do we want to do first?</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What do we need?</h2>
						<h1 style="color: white">The search algorithm!</h1>
						<aside class="notes">
							<p>Lets implement the search algorithm</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
Here is Dijkstra's algorithm as C++ code
```c++ [|6-27|1-2|19-25|22|28]
template<SearchContext C, SearchGraph G>
bool search(C& ctx, const G& graph, VertexID start, VertexID goal)
{
  ctx.reset(graph, start, goal);

  while (ctx.is_queue_not_empty())
  {
    const auto [from, to, to_dist] = ctx.dequeue();
    
    if (ctx.is_visited(to))
      continue;
    else
      ctx.mark_visited(to, from);
    
    if (ctx.is_terminal(to))
      return true;
    else
    {
      graph.for_each_edge(
        to,
        [&ctx, to, to_dist](VertexID v, const EdgeProperties& edge) mutable {
        if (edge.valid and !ctx.is_visited(v)) {
          ctx.enqueue(to, v, edge.dist + to_dist);
        }
      }); 
    }
  }
  return false;
}
```

Note:
1. Here is Dijkstra's implemented as a templated C++ function
2. Its pretty much, verbatim, the pseudo code that we just looked at, so I wont go over it line by line
3. One notable exception here is that I chose to implement the state we will be accessing during the search as an, opaque, context object of type `C` constrained by the concept `SearchContext`, and that's going to include our visited set and priority queue.
   - This context bearing object will be an important customization point; and it will allow us to re-use some allocated memory between successive search attempts.
   - Additionally, the graph object of type `G` constrained by the concept `SearchGraph`, will be our second key customization point
4. To iterate over neighboring vertices in the graph, `G` will expose a method called `for_each_edge` which will invoke a lambda with some edge property information corresponding with each connected vertex.
5. We should also notice that, in this implementation, edges not only bear a weight, but a validity flag that we will use in conjunction with the visited set to omit certain edges from the search. The purpose of this flag is to represent obstacles which would prevent the robot or agent from traversing said edge. We could imagine that our robot system might update the edges of the graph with this obstacle information right before we attempt to search.
6. As such, there might be scenarios where we can't actually find the goal. In these cases, the `search` function will simply return `false` indicating that the goal vertex was not found.

- One over arching point to make here is that we are choosing a templated function not only for the purpose genericness.
- Sure, it makes it a bit easier to reuse this function while we iterate on implementations of the aforementioned customization points
- But if we make our custom graph `G` and our context `C` header-only implementations as well, we give the compiler the ability to see the full context of the code in a single translation unit
- As such when we compile with optimizations, we give the compiler free reign to make our readable code, faster.
- We wont go into detail about exactly which optimizations are being made relative to unoptimized code, but its important to consider the fact that compiler optimizations make a huge difference on things like execution ordering and memory access, and we want to get the best out of our code without having to make these optimizations by hand before we start profiling.
					</textarea>
				</section>

				<!-- slide -->
				<section data-markdown>
					 <textarea data-template data-separator-notes="^Note:">
Here is the code to recover a path on a successful search
```c++ [|5-16]
template<typename OutputIteratorT, SearchContext C>
OutputIteratorT get_reverse_path(OutputIteratorT out, const C& ctx, VertexID from)
{
  (*out) = from;
  while (true)
  {
    if (const auto to = ctx.predecessor(from); to == from)
    {
      break;
    }
    else
    {
      (*out) = to;
      from = to;
    }
  }
  return out;
}
```

Note:
1. As a practical companion to our `search` function, we can also implement a function which uses the context object from a successful search in order to recover an optimal path back from the goal vertex
2. All it does is fill a sequence through an output iterator, like a `std::back_inserter`, with the vertex transitions we computed during the search. This sequence will represent our optimal path, but in reverse order starting from the goal
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<aside class="notes">
							<p>Ok what else should we do? As mentioned, we've established two main customization points in our `search` function.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3 style="color: white">Something to hold the graph!</h3>
						<aside class="notes">
							<p>Lets start with the graph</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold the graph!</h3>
						<lu align="left">
							<li>Vertex adjacencies (edges)</li>
						</lu>
						<aside class="notes">
							<p>The graph needs to hold adjacencies between vertices</p>
							<p>These are the edges of the graph</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold the graph!</h3>
						<lu align="left">
							<li>Vertex adjacencies (edges)</li>
							<li>Vertex properties (locations, etc.)</li>
						</lu>
						<aside class="notes">
							<p>It could probably hold some extra properties about each vertex, such as a physical location in space</p>
							<p>This wont be needed for the search, but is still useful semantic data</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold the graph!</h3>
						<lu align="left">
							<li>Vertex adjacencies (edges)</li>
							<li>Vertex properties (locations, etc.)</li>
							<li>Edge properties (distance, validity, etc.)</li>
						</lu>
						<aside class="notes">
							<p>Lastly we need to hold properties about each edge.</p>
							<p>Most importantly, a distance, as well as a flag for toggling the edge to represent accessibility</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
## What else do we need?
```c++
template <typename T>
concept SearchGraph = 
  requires(T&& g)
  {
      { g.for_each_edge(VertexID{}, [](VertexID, const EdgeProperties&) {}) };
      { g.vertex_count() };
      { g.vertex(VertexID{}) };
  };
```

Note:
- In `search`, I constrained the graph type `G` by some concept
- Here is a simple concept to check that `G` implements a few requisite methods
					</textarea>
				</section>


				<!-- slide -->
				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [|1-5|10-14|16-18]
struct VertexProperties { double x; double y; /* ... */ };

struct EdgeProperties { bool valid; EdgeWeight weight; /* ... */ };

using Edge = std::pair<VertexID, EdgeProperties>;

class Graph
{
public:
  /// Returns vertex properties for a given vertex
  const VertexProperties& vertex(VertexID q) const;

  /// Returns the total number of vertices
  std::size_t vertex_count() const;

  /// Invokes a visitor on each edge of "q"
  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const;
};
```

Note:
1. And this is a full definition of our graph without any guts
2. We have some definitions for our vertex and edge properties
3. Accessors to query about vertex properties
5. And a method for calling a visitor on each edge connected to some query vertex
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<aside class="notes">
							<p>We have one more customization point to go over</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3 style="color: white">Something to hold search state!</h3>
						<aside class="notes">
							<p>Which is the `SearchContext` object</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold search state!</h3>
						<lu align="left">
							<li>A container to hold visited vertices (and their relationships)</li>
						</lu>
						<aside class="notes">
							<p>This object hold state about which vertices have already been evaluated</p>
							<p>As well as information about which vertices we transition from to get there
							<p>Keep in mind that we will need to recover our optimal path</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>What else do we need?</h2>
						<h3>Something to hold search state!</h3>
						<lu align="left">
							<li>A container to hold visited vertices (and their relationships)</li>
							<li>A min-sorted priority queue of vertices</li>
						</lu>
						<aside class="notes">
							<p>The other big piece is our min-sorted priority queue</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
## What else do we need?
```c++
template <typename T>
concept SearchContext = 
  requires(T&& ctx)
  {
      { ctx.is_queue_not_empty() };
      { ctx.is_visited(VertexID{}) };
      { ctx.is_terminal(VertexID{}) };
      { ctx.mark_visited(VertexID{}, VertexID{}) };
      { ctx.enqueue(VertexID{}, VertexID{}, EdgeWeight{}) };
      { ctx.dequeue() };
      { ctx.predecessor(VertexID{}) };
  };
```

Note:
- Quickly, here is simple concept to check that `C` implements a few requisite methods
					</textarea>
				</section>


				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [|6-8|10-11|13-18|20-25]
struct Transition { VertexID from; VertexID to; EdgeWeight weight; };

class TerminateAtGoal
{
public:
  /// Resets internal state; sets start and goal vertices
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g);

  /// Returns true if a vertex is the goal
  bool is_terminal(VertexID q) const;
  
  /// Returns true if a vertex has been visited
  bool is_visited(VertexID q) const;
  /// Sets a vertex as visited
  void mark_visited(VertexID from, VertexID to);
  /// Returns the predecessor of a vertex
  VertexID predecessor(VertexID q) const;
	
  /// Returns true if there are still elements in the queue
  bool is_queue_not_empty() const;
  /// Get vertex from queue next best distance
  Transition dequeue();
  /// Adds vertex to queue with a distance
  void enqueue(VertexID from, VertexID to, EdgeWeight w);
};
```

Note:
1. And this is a full definition of our search context without any guts. Its a bit more involved than the graph, but each method is fairly simple
2. We need a method to reset the state of the search. This will set our start/goal vertices; reset the queue; reset the visited set... and importantly we can use it to do any up-front allocation in the containers we chose for our underlying implementation before we actually execute the search.
3. We have a way to check if a vertex is the goal vertex
3. We have some methods to mutate and access state about the visited set
4. And lastly, we have a few methods to interact with the priority queue
					</textarea>
				</section>





				<!------------------ IMPLEMENTATION 0 ------------------>
				<section>
					<h1>Implementation 0</h1>
					<aside class="notes">
						<p>With algorithm implemented, we will have to fill out the guts of the graph and search context.</p>
						<p>So we've reduced the scope of what we actually need to make choices about</p>
						<p>And more importantly, we've reduced the scope of what we will need to optimize</p>
						<p>So lets start making some choices</p>
					</aside>
				</section>


				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [|2|8|10|13-15|17-28|24-27]
#include <algorhitm> // std::for_each
#include <map>       // std::map, std::multimap
#include <tuple>     // std::apply

class Graph
{
private:
  std::multimap<VertexID, Edge> adjacencies_;

  std::map<VertexID, VertexProperties> vertices_;

public:
  const VertexProperties& vertex(VertexID q) const { return vertices_.at(q); }

  std::size_t vertex_count() const { return vertices_.size(); }

  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const
  {
    const auto [first, last] = adjacencies_.equal_range(q);
    std::for_each(
      first,
      last,
      [visitor](const auto& parent_and_edge) mutable
      {
        std::apply(visitor, parent_and_edge.second);
      });
  }
};
```

Note:
1. The first thing I'm going to implement is the `Graph`. We are going to start with a somewhat contrived implementation to serve as a jumping off point.
2. This implementation will make use of `std::map` and `std::multimap`. After all, these are the canonical, age-old associative containers in the STL, and we have a bunch of vertices that we need to associate with other vertices to represent our adjacencies in the graph.
3. We can store vertices with their neighbors using a multimap. If we consider a vertex as a key, we will need to allow for repeating keys, because each vertex will need to be associated with 1 or more neighbors. A multimap will also allow use to iterate over these neighbors nicely.
4. We can store our vertex properties as unique key-value pairs because we don't expect duplication here.
5. Our vertex property access methods use the accessors of std::map in a straight-forward way
7. And lastly, we can implement our neighbor iteration method using `std::multimap::equal_range` since it will give us access to an iterable range of all the edges associated with our query vertex `q`
8. Inside this implementation we are also using `std::for_each` from the `<algorithm>` header and `std::apply` from the `<tuple>` header. This is because the value payload associated with each node of the multimap -- our edges -- are themselves `std::pairs` and we want to invoke some user-supplied visitor callback with the elements of this pair as arguments.
					</textarea>
				</section>


				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [|1-4|9|11|13|16-27|29-63]
#include <functional>  // std::greater
#include <queue>       // std::priority_queue
#include <map>         // std::map
#include <vector>      // std::vector

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::map<VertexID, VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.clear();
    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_.emplace(to, from); }

  VertexID predecessor(VertexID q) const
  {
    if (const auto itr = visited_.find(q); itr == visited_.end())
    {
      return q;
    }
    else
    {
      return itr->second;
    }
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```

Note:
1. Ok, onto the `SearchContext` implementation
2. This time we are going to use `std::map` again. But the other major element is the `std::priority_queue` container adapter. This will remain a constant through the implementations to follow because the STL doesn't really provide any other priority queue implementation besides this one. Underneath the hood this is using `std::push_heap` and `std::pop_heap` so we know that its a heap-based queue. And we will be using a `std::vector` to store this heap simply because its the default underlying container used with a priority queue, and frankly probably the best we can do.
3. Inside the class we will store our active, current goal, which is simply a vertex ID
4. We have our min-sorted priority queue. Note here that we have to use `std::greater` as an extra comparator argument because, by default, `std::priority_queue` will sort elements from greatest to least, with greatest at the top, and this change will make it so our vertices with the least distance will appear the top of the queue to be evaluated next.
5. We will represent our visited set as an std::map which we will associate vertices, uniquely, to their predecessor vertices. The existence of a vertex in the map will signify that it has been visited.
6. In our reset method we want to make sure that any stale state is cleared. This includes flushing our visited set and our queue. As a reminder, our aim is to re-use this context object, but in this implementation re-use doesn't really buy us anything because the containers we've chosen don't have some magic reserve method to pre-allocate memory up front. This will come into play later.
7. Beyond that, the remaining methods are fairly straight-forward, and merely serve as wrappers around calls to methods of the underlying containers
					</textarea>
				</section>
				<!------------------ IMPLEMENTATION 0 ------------------>





				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<aside class="notes">
							<p>Now that we have all the pieces, lets measure the performance of our search.
							<p>Specifically, we will be looking at performance with respect to cache interactions, because, as we claimed initially, this will have a big impact on overall performance.</p>
							<p>At least for this first implementation, lets not expect too much.</p>
						</aside>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						<textarea data-template>
## Measuring cache performance

- Run searches over our example warehouse graph

Note:
- In order to do this, we will run a program which will load the spatial warehouse graph from before
- And then use our search implementation to run searches between vertices in this graph
						</textarea>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						<textarea data-template>
## Measuring cache performance

- Run searches over our example warehouse graph
- Select some percentage of all vertices, N

Note:
- We can pretend that this behavior is like a request that the robot is making to a service that would compute one or more path plans at a time
- Of course, to benchmark, we are running many more plans at a time than a robot would actually need
- But lets suspend our disbelief a bit
						</textarea>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						<textarea data-template>
## Measuring cache performance

- Run searches over our example warehouse graph
- Select some percentage of all vertices, N
- Run N^2 searches between all vertices as start/goal pairs

Note:
- This will give us a good idea of the performance of our search implementation over a mixture of short range and long range plans
- Additionally, on the back-end, I've randomized how the vertices are arranged in the graph after they are loaded from a file to normalize the affects of lucky placement in memory
- We aren't trying to have any hand-crafted control over memory placement and it would be good to avoid biasing our results with some lucky ordering the best we can
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h3 style="color: white">Run under cachegrind</h3>
						<aside class="notes">
							<p>One tool I like to use to get a rough idea of cache performance is called `cachegrind`</p>
							<p>Its particularly useful for short-running programs like this one</p>
						</aside>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						<textarea data-template>
## Measuring cache performance

### Run under cachegrind

```bash
> valgrind --tool=cachegrind --cache-sim=yes ./run_search ...
```	

Note:
- Its a tool which can be invoked with `valgrind`
- It instruments our compiled code before execution, which will slow execution down significantly
- But it gives us a few extremely useful performance indicators
- These includes the number of instructions run during the program
- As well as an estimate about cache performance
- It does so by simulating the cache
- This simulation assumes an architecture with two cache levels
- In the case of modern processors with more than two cache levels, `cachegrind` will detect this, and
    - simulate the smallest and largest instruction cache
    - as well as the smallest and largest data cache
						</textarea>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						 <textarea data-template>
## Measuring cache performance
```text [|4-8|10-14]
==131579== Cachegrind, a cache and branch-prediction profiler
==131579== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
…
==131579== I   refs:      20,721,901,316
==131579== I1  misses:             4,744
==131579== LLi misses:             3,909
==131579== I1  miss rate:           0.00%
==131579== LLi miss rate:           0.00%
==131579== 
==131579== D   refs:       7,762,293,651  (6,285,352,547 rd   + 1,476,941,104 wr)
==131579== D1  misses:       615,102,179  (  604,712,021 rd   +    10,390,158 wr)
==131579== LLd misses:         3,336,167  (    2,266,653 rd   +     1,069,514 wr)
==131579== D1  miss rate:            7.9% (          9.6%     +           0.7%  )
==131579== LLd miss rate:            0.0% (          0.0%     +           0.1%  )
==131579== 
==131579== LL refs:          615,106,923  (  604,716,765 rd   +    10,390,158 wr)
==131579== LL misses:          3,340,076  (    2,270,562 rd   +     1,069,514 wr)
==131579== LL miss rate:             0.0% (          0.0%     +           0.1%  )
```

Note:
1. Here is a summary of the cache performance of our first implementation
2. First off, we can look at the instruction cache simulation results. We notice a very large number of references to our instruction caches, but the actual number of instruction cache misses for both levels is very low. So low that we see a virtually a 0% miss rate. This is great. For this program, as well as the variants to follow, we wont really need to worry about our instruction cache much. What this tells us is that our program fits nicely within the L1 instruction cache and is basically never evicted.
3. On the other hand, our data cache is a whole other story. We see that, our simulated last-level (LL) cache experienced virtually no misses, but our L1 cache -- the one closest to the CPU -- experienced a very large number of misses. In fact almost 10% of all L1 cache references resulted in a miss

- Here, cache-grind has given us indication that our program is very likely to be bottlenecked by memory accessed through the cache
- To proceed, I like to use another tool to measure the actual performance of the program based on hardware performance counters.
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h3 style="color: white">perf</h3>
						<aside class="notes">
							<p>The program I use most regularly is `perf`, which is available on most linux-based systems.</p>
							<p>I'm pretty confident in saying that most of us in the robotics world are using some kind of linux distro so its safe to assume this is available for most within the domain.</p>
							<p>To my knowledge, there are similar tools for Windows, but I just haven't use them</p>
						</aside>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						<textarea data-template>
## Measuring cache performance

### perf

```bash
> sudo apt install linux-tools-6.2.0-33-generic

> perf record \
 -o ~/perf.data \
 --call-graph dwarf \
 --event instructions,cpu-cycles,cache-misses,branches,branch-misses \
 --aio \
 --sample-cpu <exec>
```	

Note:
- Perf is what is referred to as a sampling-based profiler
- What it will do is record the state of hardware counters during the execution of our program at some designated rate
- It can give us insight into like the number of cpu-cycles, instructions, cache-misses, branches, branch-misses, and more
- And it can also be attached to a live programs
- The problem with perf, in my opinion, is that the output is hard to visually parse
						</textarea>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						<textarea data-template>
## Measuring cache performance

### perf

```bash
> sudo apt install hotspot

> sudo hotspot
```	

Note:
- So, to look at the results that perf generates, we can use this nice program called `hotspot` which, among other things, will produce interactive flamegraphs of particular events to show us which calls contributed the most of a particular event
- We will look at instruction counts
- And cache miss counts of particular function calls to figure out exactly which functions need to be optimized the most
						</textarea>
					</section>
					<section data-auto-animate data-markdown data-visibility="hidden">
						<textarea data-template>
## Measuring cache performance

### Instructions

![me](slides/v0_hotspot_instructions.png)
						</textarea>
					</section>
					<section data-auto-animate data-markdown data-visibility="hidden">
						<textarea data-template>
## Measuring cache performance

### Cache-misses

![me](slides/v0_hotspot_cache_misses.png)
						</textarea>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<aside class="notes">
							<p>So now that we have some measurements, how do we actually improve the cache performance of our path planner</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Spatial Locality</h3>
						<aside class="notes">
							<p>A core tenant to good cache performance is what is refereed to as: spatial locality</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Spatial Locality</h3>
						<lu align="left">
							<li>Data that we access should be close together in memory.</li>
						</lu>
						<aside class="notes">
							<p>In essence this means that we should try to keep that data that we will access close together in memory</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Temporal Locality</h3>
						<aside class="notes">
							<p>In addition, we should strive to achieve good temporal locality</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Temporal Locality</h3>
						<lu align="left">
							<li>Maximize the number of times we access data in the cache.</li>
						</lu>
						<aside class="notes">
							<p>Meaning that we should try to construct our program such that we are maximizing the amount of times we are accessing data that is already in the cache</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Rules of thumb</h3>
						<aside class="notes">
							<p>Some good rules of thumb for achieving both of things things include...</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Rules of thumb</h3>
						<lu align="left">
							<li>Access data sequentially.</li>
						</lu>
						<aside class="notes">
							<p>Laying out out data so that it can be accessed sequentially</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Rules of thumb</h3>
						<lu align="left">
							<li>Access data sequentially.</li>
							<li>Reduce indirections through pointers.</li>
						</lu>
						<aside class="notes">
							<p>Reducing the number of pointer indirections in our programs</p>
							<p>more specifically: we will want to avoid having to de-reference pointers to access data in any linked node structures as much as possible</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Rules of thumb</h3>
						<lu align="left">
							<li>Access data sequentially.</li>
							<li>Reduce indirections through pointers.</li>
							<li style="color: cyan">Don't allocate memory during execution.</li>
						</lu>
						<aside class="notes">
							<p>And sort of a related big one: we want avoid memory allocations during execution of our hottest code paths.</p>
							<p>I think that this is a pretty well known thing that the mechanisms of memory allocations can be relatively slow</p>
							<p>This is because memory allocation, itself, is a non-trivial operation</p>
							<p>But on top of that, successive calls to a general purpose memory allocator through the `new` operation -- which happen when we add elements to our STL containers -- might place those elements in non sequential areas of memory</p>
							<p>So we at the mercy of malloc when it comes to placement unless we write our own container allocators</p>
							<p>But I'm claiming here that we can get pretty far without needing to do so</p>
						</aside>
					</section>

					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3 style="color: white">Does out first implementation do any of these things?</h3>
						<aside class="notes">
							<p>Looking back out our first pass at an implementation:</p>
							<p>Does it achieve most or any of these guiding principals</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Achieving good (data) cache performance</h2>
						<h3>Does out first implementation do any of these things?</h3>
						<h1 style="color: white">No not really.</h3>
						<aside class="notes">
							<p>The simple answer is: no, not really</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::multimap</h4>
						<aside class="notes">
							<p>So lets take a closer look</p>
							<p>When we looked at out results in hotspot, we noticed that a large contributer to the total cache misses in our program were method associated with std::multimap</p>
							<p>Why is this so?</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::multimap</h4>
						<div class="r-stack">
							<img src="slides/multimap_mental_model.png">
						</div>
						<aside class="notes">
							<p>std::multimap is holding the adjacencies in our graph</p>
							<p>And we are walking over the adjacencies of a vertex at each iteration of Dijkstra's</p>
							<p>Mentally, we can model these associations with a diagram that looks like so, where vertices are associated with sequences edge data</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::multimap</h4>
						<div class="r-stack">
							<img src="slides/multimap_actual.png">
						</div>
						<aside class="notes">
							<p>In actuality, as we could see in some of our `perf` outputs, std::multimap is actually implement as a red-block tree, at least in GCC and Clang</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::multimap</h4>
						<div class="r-stack">
							<img src="slides/multimap_mem.png">
						</div>
						<aside class="notes">
							<p>The nodes of this tree are linked via pointers, and the nodes themselves are placed in memory by an allocator</p>
							<p>Since we have no control over where these node are placed -- unless we write out own allocator -- they can be placed anywhere ... we have no know way of knowing</p>
							<p>Sometimes this means, even when iterating through an adjacency list, we could be hopping around memory quite a bit</p>
							<p>Its even possible that we are creating cache-misses every time we iterate to another element in the adjacency sequence, simply because adjacent nodes were placed in such a way that they never end up in the same cache-line</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::multimap::equal_range(4)</h4>
						<div class="r-stack">
							<img src="slides/multimap_equal_range.png">
						</div>
						<aside class="notes">
							<p>In the case of std::multimap, however, the story is even a bit worse than that</p>
							<p>Finding the values associated with a key in std::map or std::multimap involves walking the tree until we find a node with the matching key</p>
							<p>std::multimap::equal_range is, as many of us know, guaranteed to be a O(log(n)) operation</p>
							<p>This means that, even beyond iterating through our adjacencies, we are jumping between nodes just to find our adjacencies to begin with</p>
							<p>Since our `search` naturally needs to query the std::multimap rather often, we find ourselves with a rather pathological case</p>
						</aside>
					</section>
				</section>



				<!------------------ IMPLEMENTATION 1 ------------------>
				<section>
					<h1>Implementation 1</h1>
					<aside class="notes">
						<p>std::multimap, has its uses, especially in cases which require key-value pairs to be in a particular order when iterating</p>
						<p>But here we don't really have a need for those properties</p>
						<p>The good news is that, as many of you are already probably guessing, we can improve things pretty immediately with a container which doesn't affect the structure of our existing implementation much at all</p>
					</aside>
				</section>


				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [3,8,10|13-27]
#include <algorithm>     // std::for_each
#include <tuple>         // std::apply
#include <unordered_map> // std::unordered_map, std::unordered_multimap

class Graph
{
private:
  std::unordered_multimap<VertexID, Edge> adjacencies_;

  std::unordered_map<VertexID, VertexProperties> vertices_;

public:
  const VertexProperties& vertex(VertexID q) const { return vertices_.at(q); }

  std::size_t vertex_count() const { return vertices_.size(); }

  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const
  {
    const auto [first, last] = adjacencies_.equal_range(q);
    std::for_each(
      first,
      last,
      [visitor](const auto& parent_and_edge) mutable
      {
        std::apply(visitor, parent_and_edge.second);
      });
  }
};
```

Note:
1. In this updated implementation, we will be replacing all instances of std::map and std::multimap with their "unordered" counterparts
2. Because this is, functionally, a drop-in replacement, the structure of our implementation remains exactly the same as before
					</textarea>
				</section>


				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [3,13]
#include <functional>
#include <queue>
#include <unordered_map>
#include <vector>

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::unordered_map<VertexID, VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.clear();
    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_.emplace(to, from); }

  VertexID predecessor(VertexID q) const
  {
    if (const auto itr = visited_.find(q); itr == visited_.end())
    {
      return q;
    }
    else
    {
      return itr->second;
    }
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```
Note:
1. Likewise, in our `SearchContext` implementation we will swap out std::map used to represent our visited set for a std::unordered_map
					</textarea>
				</section>
				<!------------------ IMPLEMENTATION 1 ------------------>


				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						 <textarea data-template>
## Measuring cache performance
```text [10-14|4]
==132432== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==132432== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==132432== I   refs:      11,574,779,293
==132432== I1  misses:             3,778
==132432== LLi misses:             3,700
==132432== I1  miss rate:           0.00%
==132432== LLi miss rate:           0.00%
==132432== 
==132432== D   refs:       4,525,697,105  (2,946,691,588 rd   + 1,579,005,517 wr)
==132432== D1  misses:       109,346,888  (  103,780,511 rd   +     5,566,377 wr)
==132432== LLd misses:         3,231,301  (    2,205,723 rd   +     1,025,578 wr)
==132432== D1  miss rate:            2.4% (          3.5%     +           0.4%  )
==132432== LLd miss rate:            0.1% (          0.1%     +           0.1%  )
==132432== 
==132432== LL refs:          109,350,666  (  103,784,289 rd   +     5,566,377 wr)
==132432== LL misses:          3,235,001  (    2,209,423 rd   +     1,025,578 wr)
==132432== LL miss rate:             0.0% (          0.0%     +           0.1%  )
```
Note:
1. Measuring again with `cachegrind` we see that our D1 cache-miss rate has gone down rather significantly
2. I also want to point out that on top of our cache miss rate, the total number of instructions executed during our program has decreased by about half. Most of this has to do with the fact that the data access complexity of std::unorder_map is much lower than that of std::map.
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/simulated_cm_over_i_v1.png">
						</div>
						<aside class="notes">
							<p>Here is a graph showing our simulated D1 cache-miss counts normalized by the total number of instruction references encountered during the program between our previous implementation and this one, side by side</p>
							<p>We see that by making this very simple change, we've improved our data cache-performance significantly</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/cycle_counts_v1.png">
						</div>
						<aside class="notes">
							<p>Its important to also show that, in terms of cycle-counts reported by perf, this implementation is also faster than our previous implementation by leaps and bounds</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h4>std::unordered_multimap</h4>
						<div class="r-stack">
							<img src="slides/unordered_multimap_mental_model.png">
						</div>
						<aside class="notes">
							<p>To dig a bit deeper, lets look at why this might be in terms of our guiding principals</p>
							<p>Again, here is a mental-model for how our graph data is organized</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h4>std::unordered_multimap</h4>
						<div class="r-stack">
							<img src="slides/unordered_multimap_actual.png">
						</div>
						<aside class="notes">
							<p>std::unordered_map is, effectively, a hashmap</p>
							<p>This means that the data structure itself is likely comprised of an array of buckets, each of which point to some kind of linked list or tree structure</p>
							<p>Because multiple keys can be resolved to the same bucket after hashing, access will involved walking these linked structures to find a matching key by an equality comparison</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h4>std::unordered_multimap</h4>
						<div class="r-stack">
							<img src="slides/unordered_multimap_mem.png">
						</div>
						<aside class="notes">
							<p>Similarly to std::map, the nodes attached to a bucket are placed in memory by an allocator, so in the default case, there is no guarantee that they are adjacent</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h4>std::unordered_multimap::equal_range(113)</h4>
						<div class="r-stack">
							<img src="slides/unordered_multimap_equal_range.png">
						</div>
						<aside class="notes">
							<p>We have definitely reduced the total number of indirections, and therefore potential cache-misses, during a call to `equal_range` by making this change</p>
							<p>However, in ordered to find a vertex we must still follow the links between nodes in a linked list</p>
							<p>Additionally, we will need to iterate over a linked list when traversing the adjacencies associated with a vertex</p>
						</aside>
					</section>
				</section>


				<!------------------ IMPLEMENTATION 1.5 ----------------->
				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [20-21]
#include <functional>     // std::greater
#include <queue>          // std::priority_queue
#include <unordered_map>  // std::unordered_map
#include <vector>         // std::vector

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::unordered_map<VertexID, VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.clear();
    visited_.reserve(graph.vertex_count());
    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return visited_.count(q); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_.emplace(to, from); }

  VertexID predecessor(VertexID q) const
  {
    if (const auto itr = visited_.find(q); itr == visited_.end())
    {
      return q;
    }
    else
    {
      return itr->second;
    }
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```

Note:
- One simple change we can make is to ensure that each bucket only every contains data associated with a single vertex
- To do so we can call reserve on our `std::unordered_map`s and `std::unordered_multimap`s to manually set the number of buckets.
					</textarea>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						 <textarea data-template>
## Measuring cache performance
```text [10-14]
==133350== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==133350== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==133350== I   refs:      11,574,779,289
==133350== I1  misses:             6,017
==133350== LLi misses:             3,935
==133350== I1  miss rate:           0.00%
==133350== LLi miss rate:           0.00%
==133350== 
==133350== D   refs:       4,525,697,103  (2,946,691,588 rd   + 1,579,005,515 wr)
==133350== D1  misses:        90,999,080  (   85,486,438 rd   +     5,512,642 wr)
==133350== LLd misses:         3,231,302  (    2,205,724 rd   +     1,025,578 wr)
==133350== D1  miss rate:            2.0% (          2.9%     +           0.3%  )
==133350== LLd miss rate:            0.1% (          0.1%     +           0.1%  )
==133350== 
==133350== LL refs:           91,005,097  (   85,492,455 rd   +     5,512,642 wr)
==133350== LL misses:          3,235,237  (    2,209,659 rd   +     1,025,578 wr)
==133350== LL miss rate:             0.0% (          0.0%     +           0.1%  )

```

Note:
- We see from our cache-performance simulation that this has a non-trivially positive effect on our cache-miss rate
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/simulated_cm_over_i_v1_res.png">
						</div>
							<aside class="notes">
							<p>Here is another side-by-side comparison to make it more obvious</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/cycle_counts_v1_res.png">
						</div>
						<aside class="notes">
							<p>And our cycle counts reported by perf, have also reduced, but only by a smidge</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h4>std::unordered_multimap</h4>
						<div class="r-stack">
							<img src="slides/unordered_multimap_mem_bins_reserved.png">
						</div>
						<aside class="notes"/>
							<p>What we are doing here is exploiting the knowledge that our keys are simple integers</p>
							<p>And the fact that these keys are distributed over a range of values from 0 to N-1</p>
							<p>So their hash value can simply be the value of the keys themselves with no hash collisions</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<h4>std::unordered_multimap::equal_range(113)</h4>
						<div class="r-stack">
							<img src="slides/unordered_multimap_equal_range_bins_reserved.png">
						</div>
						<aside class="notes"/>
							<p>This is does not, however, affect iterating over adjacencies associated with a given vertex</p>
							<p>So if our vertex has a large number of adjacencies, we are likely to still see a fair number of cache-misses</p>
						</aside>
					</section>
				</section>
				</section>
				<!------------------ IMPLEMENTATION 1.5 ----------------->





				<!------------------ IMPLEMENTATION 2 ------------------>
				<section>
					<h1>Implementation 2</h1>
					<aside class="notes"/>
						<p>And that brings, rather naturally, to our final major variation</p>
					</aside>
				</section>


				<section data-auto-animate data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [|3,8-10|19-27]
#include <algorithm> // std::for_each
#include <tuple>     // std::apply
#include <vector>    // std::vector

class Graph
{
private:
  std::vector<VertexProperties> vertices_;

  std::vector<std::vector<Edge>> adjacencies_;

public:
  const VertexProperties& vertex(VertexID q) const { return vertices_[q]; }

  std::size_t vertex_count() const { return vertices_.size(); }

  template<typename EdgeVisitorT>
  void for_each_edge(VertexID q, EdgeVisitorT&& visitor) const
  {
    std::for_each(
      adjacencies_[q].begin(),
      adjacencies_[q].end(),
      [q, visitor](const auto& child_and_edge_weight) mutable
      {
        const auto& [succ, edge_weight] = child_and_edge_weight;
        visitor(succ, edge_weight);
      });
  }
};
```

Note:
1. std::vector is almost always the best choice, and we can't escape it
2. Here we are swapping out our associative containers for a simple std::vector. Given the chosen representation for our keys, we really don't need anything more complicated than this. 
3. To reflect these changes, we need to update our `for_each_edge` implementation a bit, but notice that the structure remains basically the same

					</textarea>
				</section>


				<section data-auto-animate data-markdown data-markdown data-separator-notes="^Note:">
					 <textarea data-template>
```c++ [3,12|19-20|32,36]
#include <functional> // std::greater
#include <queue>      // std::priority_queue
#include <vector>     // std::vector

class TerminateAtGoal
{
private:
  VertexID goal_;

  std::priority_queue<Transition, std::vector<Transition>, std::greater<Transition>> queue_;

  std::vector<VertexID> visited_;

public:
  template<SearchGraph G>
  void reset(G&& graph, VertexID s, VertexID g)
  {
    // Clear visited set
    visited_.resize(graph.vertex_count());
    visited_.assign(graph.vertex_count(), graph.vertex_count());

    // Clear any stragglers in the queue
    while (!queue_.empty()) { queue_.pop(); }
    // Set current goal
    goal_ = g;
    // Add start as first vertex in queue
    enqueue(s, s, 0);
  }

  bool is_queue_not_empty() const { return !queue_.empty(); }

  bool is_visited(VertexID q) const { return q != visted_.size(); }

  bool is_terminal(VertexID q) const { return goal_ == q; }

  void mark_visited(VertexID from, VertexID to) { visited_[to] = from; }

  VertexID predecessor(VertexID q) const
  {
    return visited_[q];
  }

  Transition dequeue()
  {
    auto t = queue_.top();
    queue_.pop();
    return t;
  }

  void enqueue(VertexID from, VertexID to, EdgeWeight w)
  {
    queue_.push(Transition{
      .from = from,
      .to = to,
      .weight = w
    });
  }
};
```

Note:
1. Likewise, in our search context implementation, we can change the visited set to be represented by a std::vector as well
2. The only real functional difference here is that we need to represent an unvisited vertex in a slightly different way. Previously, we were checking for the existence of a key in an a map. But here we need to assign each unvisited vertex with some special value. Because the our vertex IDs span from 0 to N-1, we can just chose this value to be N, the total number of vertices in the graph
3. This way when we go check to see if a vertex has been visited, all we have to do is check to see if its predecessor vertex ID has been set to to something other than N
					</textarea>
				</section>
				<!------------------ IMPLEMENTATION 2 ------------------>


				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						 <textarea data-template>
## Measuring cache performance
```text [10-14]
==135295== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==135295== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==135295== I   refs:      6,919,489,865
==135295== I1  misses:            6,058
==135295== LLi misses:            3,887
==135295== I1  miss rate:          0.00%
==135295== LLi miss rate:          0.00%
==135295== 
==135295== D   refs:      2,516,605,627  (1,700,911,074 rd   + 815,694,553 wr)
==135295== D1  misses:       43,267,746  (   40,602,167 rd   +   2,665,579 wr)
==135295== LLd misses:        3,054,946  (    2,183,488 rd   +     871,458 wr)
==135295== D1  miss rate:           1.7% (          2.4%     +         0.3%  )
==135295== LLd miss rate:           0.1% (          0.1%     +         0.1%  )
==135295== 
==135295== LL refs:          43,273,804  (   40,608,225 rd   +   2,665,579 wr)
==135295== LL misses:         3,058,833  (    2,187,375 rd   +     871,458 wr)
==135295== LL miss rate:            0.0% (          0.0%     +         0.1%  )

```

Note:
- As expected our cache-miss rate has decreased yet again
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/simulated_cm_over_i_v2.png">
						</div>
							<aside class="notes">
							<p>When we look at the numbers side by side, we see we have made steady progress in reducing our D1 cache misses</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/cycle_counts_v2.png">
						</div>
						<aside class="notes">
							<p>And the speed of our program has again increased significantly</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::vector::operator[] -> std::vector</h4>
						<div class="r-stack">
							<img src="slides/vector_vector_mental_model.png">
						</div>
						<aside class="notes"/>
							<p>Looking just at our graph structure again...</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Let's take a closer look</h2>
						<h4>std::vector::operator[] -> std::vector</h4>
						<div class="r-stack">
							<img src="slides/vector_vector_access.png">
						</div>
						<aside class="notes"/>
							<p>We know that by using a vector of vectors to hold our adjacencies, we still have two indirections happening every time we need to access the adjacencies associated with a given vertex</p>
							<p>but we are at least guaranteed that adjacency data will be laid out as a contiguous sequence.</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<aside class="notes"/>
							<p>One last thing that I would like to show is that we can influence cache performance just by changing the overall ordering of vertices and vertex data within the graph</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can re-order vertices and their adjacencies with std::sort</li>
						</lu>
						<aside class="notes"/>
							<p>The STL provides with some facilities to sort things, namely std::sort</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can re-order vertices and their adjacencies with std::sort</li>
							<li>If we can find a single sort which works for many start and goal pairs, then its practical to do</li>
						</lu>
						<aside class="notes"/>
							<p>If we can find a single vertex sort run just once after loading the grap from disk, then its probably a pretty practical thing to try</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can re-order vertices and their adjacencies with std::sort</li>
							<li>If we can find a single sort which works for many start and goal pairs, then its practical to do</li>
							<li>We know that vertices which are close in space are more likely to be neighbors</li>
						</lu>
						<aside class="notes"/>
							<p>By looking at the graph, we notice that vertices which are closer together are more likely to be neighbors</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Impact of ordering of the graph</h2>
						<lu	align="left">
							<li>We can order vertex adjacency data in memory such that the probability of accessing nearby vertex data in the same cache line is higher</li>
						</lu>
						<aside class="notes"/>
							<p>So why not try to sort our vertices so that when we go to access neighbor data, there is a higher probability that its already in the same cache line</p>
						</aside>
					</section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						 <textarea data-template>
## Impact of ordering of the graph
```c++ [|1-3|5-13|12]
std::vector<std::size_t> remapping;
remapping.resize(graph.vertex_count());
std::iota(remapping.begin(), remapping.end(), 0);

std::sort(
  remapping.begin(),
  remapping.end(),
  [&graph](VertexID lhs, VertexID rhs) -> bool
  {
    const auto& lv = graph.vertex(lhs);
    const auto& rv = graph.vertex(rhs);
    return ((lv.x * lv.x) + (lv.y + lv.y)) < ((rv.x * rv.x) + (rv.y + rv.y));
  });

graph.remap(remapping);
```

Note:
1. To do this, we can finally make use of our vertex property data, namely the position of each vertex in the map our environment
2. We create a sequence of indices that we can use to remap vertex data, as well as edge adjacency data
3. And then sort these indices using a custom comparison operation which sensitive to the spatial location of each vertex
4. Here, as an example, I've just sorted vertices based on their squared L2 distance from the map origin, which is sort of arbitrary, but seems to work fairly well
						</textarea>
					</section>
				</section>



				<!-- slide -->
				<section>
					<section data-auto-animate data-markdown data-separator-notes="^Note:">
						 <textarea data-template>
## Measuring cache performance
```text [10-14]
==135709== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.
==135709== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
…
==135709== I   refs:      6,944,720,850
==135709== I1  misses:            6,115
==135709== LLi misses:            3,906
==135709== I1  miss rate:          0.00%
==135709== LLi miss rate:          0.00%
==135709== 
==135709== D   refs:      2,526,314,915  (1,707,693,876 rd   + 818,621,039 wr)
==135709== D1  misses:       28,448,985  (   25,770,132 rd   +   2,678,853 wr)
==135709== LLd misses:        3,054,896  (    2,183,489 rd   +     871,407 wr)
==135709== D1  miss rate:           1.1% (          1.5%     +         0.3%  )
==135709== LLd miss rate:           0.1% (          0.1%     +         0.1%  )
==135709== 
==135709== LL refs:          28,455,100  (   25,776,247 rd   +   2,678,853 wr)
==135709== LL misses:         3,058,802  (    2,187,395 rd   +     871,407 wr)
==135709== LL miss rate:            0.0% (          0.0%     +         0.1%  )

```

Note:
- Again, here are our data cache misses
						</textarea>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/simulated_cm_over_i_v2_sorted.png">
						</div>
							<aside class="notes">
							<p>Again, looking at the numbers side by side we see another significant reduction in cache misses</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Measuring cache performance</h2>
						<div class="r-stack">
							<img src="slides/cycle_counts_v2_sorted.png">
						</div>
						<aside class="notes">
							<p>And a faster program, overall, as well</p>
						</aside>
					</section>
				</section>


				<!-- slide -->
				<section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
							</lu>
						</div>
						<aside class="notes">
							<p>Cache performance IS performance. Whether its high-frequency trading or a robot path planning, its the same sort of hardware, so being conscious about our memory-sensitive choices matters.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
							</lu>
						</div>
						<aside class="notes">
							<p>I've shown that we can make a good dent in our path planning performance before we even make any algorithmic optimizations.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
								<li>We can use the STL to effectively implement a SBP algorithm</li>
							</lu>
						</div>
						<aside class="notes">
							<p>In doing so, we can make ample of the STL to drive these implementations.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
								<li>We can use the STL to effectively implement a SBP algorithm</li>
								<li>Memory ordering has a large impact (but we do not need to jump straight to writing allocators)</li>
							</lu>
						</div>
						<aside class="notes">
							<p>And we can use it effectively before even considering to write our own allocation strategies.</p>
						</aside>
					</section>
					<section data-auto-animate>
						<h2>Some final remarks</h2>
						<div align="left">
							<lu>
								<li>Cache performance has a non-trivial effect on any algorithm</li>
								<li>We can optimize the code for our search based planning (SBP) algorithms before even optimizing the algorithm itself and make an appreciable performance impact</li>
								<li>We can use the STL to effectively implement a SBP algorithm</li>
								<li>Memory ordering has a large impact (but we do not need to jump straight to writing allocators)</li>
								<li>Cache pipelines are complex, so measurement is our best bet!</li>
							</lu>
						</div>
						<aside class="notes">
							<p>Lastly, our hardware is complex; our cache-pipelines are complex; so like many have said before me: measurement is you best bet for guiding optimization.</p>
						</aside>
					</section>


				<!-- slide -->
				<section>
					<h2>Thanks!</h2>
					<h3 style="color: white">https://github.com/briancairl/cppcon2023</h3>
				<second>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: 'c/t',
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
